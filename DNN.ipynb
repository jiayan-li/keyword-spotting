{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F  \n",
    "from DNN_utils import (flatten, column_str_to_numpy, check_accuracy, check_loss)\n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from joblib import load \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import helper functions.\n",
    "import mfcc_label \n",
    "import get_prob\n",
    "\n",
    "# Read the data\n",
    "df_train_val = pd.read_csv('processed_data/dnn_never_train.csv')\n",
    "df_test = pd.read_csv('processed_data/dnn_never_test.csv')\n",
    "\n",
    "# Some columns are recorded as string although they are arrays.\n",
    "column_str_to_numpy(df_train_val, 'mfcc')\n",
    "column_str_to_numpy(df_train_val, 'label')\n",
    "column_str_to_numpy(df_test, 'mfcc')\n",
    "column_str_to_numpy(df_test, 'label')\n",
    "\n",
    "#Split the train set into train and validation sets.\n",
    "df_train_pre, df_val = train_test_split(df_train_val, test_size=0.2, random_state=42)\n",
    "df_train_pre.reset_index(drop=True, inplace=True)\n",
    "df_val.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Create a single class label (type: int) which is the highest probability class in the label vector (type: 14x1 array).\n",
    "df_train_pre['single_class_label'] = df_train_pre['label'].apply(lambda x: np.argmax(x))\n",
    "df_val['single_class_label'] = df_val['label'].apply(lambda x: np.argmax(x))\n",
    "df_test['single_class_label'] = df_test['label'].apply(lambda x: np.argmax(x))\n",
    "\n",
    "# Configurations \n",
    "#NUM_TRAIN = int(0.8*len(df_train_pre)) # Number of training examples for splitting training and validation datasets. \n",
    "#NUM_ROWS = len(df_train_pre)\n",
    "device = 'cpu'\n",
    "dtype = torch.float32\n",
    "\n",
    "# DNN Architecture Hyperparameters\n",
    "minibatch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15493\n",
      "12394\n",
      "3099\n"
     ]
    }
   ],
   "source": [
    "print(len(df_train_val))\n",
    "print(len(df_train_pre))\n",
    "print(len(df_val)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Upsample from observations that give positive probability on one of the 12 classes that correspond to 'never'. \n",
    "def upsample_minority(df, mask):\n",
    "    df_minority = df[mask]\n",
    "    df_majority = df[~mask]\n",
    "\n",
    "    df_minority_upsampled = resample(df_minority,\n",
    "                                    replace=True,     # sample with replacement\n",
    "                                    n_samples=len(df_majority),    # to match majority class\n",
    "                                    random_state=42) # reproducible results\n",
    "\n",
    "    df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "    return df_upsampled\n",
    "\n",
    "mask = df_train_pre['label'].apply(lambda x: any(elem > 0 for elem in x[:12]))\n",
    "#mask = df_train_pre['label'].apply(lambda x: all(elem == 0 for elem in x[12:]))\n",
    "df_train = upsample_minority(df_train_pre, mask)\n",
    "\n",
    "print('Before upsampling:')\n",
    "print(df_train_pre['label'].apply(lambda x: any(elem > 0 for elem in x[:12])).value_counts()) \n",
    "\n",
    "# Display new class counts\n",
    "print('After upsampling:')\n",
    "print(df_train['label'].apply(lambda x: any(elem > 0 for elem in x[:12])).value_counts()) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train['single_class_label'].value_counts())\n",
    "\n",
    "# Step 1: Sample from df_filtered such that \n",
    "\n",
    "#Questions:\n",
    "#1- MFCC's that correspond to the same phoneme are closer in the feature space.\n",
    "#   Should we take advantage of this while oversampling? (e.g. taking convex combination of two observations.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load('processed_data/train_test_dataset_never.joblib')['test'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset into a format that torch can read.\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None, train=True):\n",
    "        # Convert the DataFrame to tensors or appropriate formats initially\n",
    "        self.mfcc = torch.tensor(np.vstack(dataframe['mfcc'].to_list()), dtype=torch.float32)\n",
    "        self.label = torch.tensor(np.vstack(dataframe['label'].to_list()), dtype=torch.long)\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mfcc)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mfcc = self.mfcc[idx]\n",
    "        label = self.label[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            mfcc = self.transform(mfcc)\n",
    "\n",
    "        return mfcc, label\n",
    "\n",
    "# Create an instance of your dataset with your DataFrame\n",
    "dataset_train = CustomDataset(df_train, train=True)  # Assuming df is your pandas DataFrame\n",
    "dataset_val = CustomDataset(df_val, train=True)\n",
    "dataset_test = CustomDataset(df_test,train=False)\n",
    "\n",
    "\n",
    "# Create the DataLoader to handle batching\n",
    "loader_train = DataLoader(dataset_train, batch_size=minibatch_size,\n",
    "                          sampler=sampler.SubsetRandomSampler(range(len(df_train))))\n",
    "\n",
    "loader_val = DataLoader(dataset_val, batch_size=1,\n",
    "                        sampler=sampler.SequentialSampler(range(len(df_val))))\n",
    "\n",
    "loader_test = DataLoader(dataset_test, batch_size=1,\n",
    "                        sampler=sampler.SequentialSampler(range(len(df_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code to print the contents of the first few batches in loader_train\n",
    "\n",
    "for i, (inputs, labels) in enumerate(loader_train):\n",
    "    print(f\"Batch {i + 1}\")\n",
    "    print(f\"Features (MFCCs) size: {inputs.size()}\")\n",
    "    print(f\"Labels size: {labels.size()}\")\n",
    "    print(\"\\n\") \n",
    "    \n",
    "    # Optional: Stop after a few batches to avoid flooding the output\n",
    "    if i == 2:  # Adjust this number based on how many batches you want to see\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_FC(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "        # assign layer objects to class attributes\n",
    "        # We may write a loop if we use the same activation function for all layers.\n",
    "        self.fc1 = nn.Linear(input_size, input_size)\n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        self.fc2 = nn.Linear(input_size, input_size)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight) \n",
    "        self.fc3 = nn.Linear(input_size, input_size)\n",
    "        nn.init.kaiming_normal_(self.fc3.weight)\n",
    "        self.fc4 = nn.Linear(input_size, input_size)\n",
    "        nn.init.kaiming_normal_(self.fc4.weight)\n",
    "        self.fc5 = nn.Linear(input_size, num_classes)\n",
    "        nn.init.kaiming_normal_(self.fc5.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_temp = x\n",
    "        x_temp = flatten(x_temp)\n",
    "        x_temp = F.relu(self.fc1(x_temp))\n",
    "        x_temp = F.relu(self.fc2(x_temp))\n",
    "        x_temp = F.relu(self.fc3(x_temp))\n",
    "        x_temp = F.relu(self.fc4(x_temp))\n",
    "        scores = self.fc5(x_temp)\n",
    "        return scores\n",
    "\n",
    "\n",
    "def test_DNN_FC():\n",
    "    input_size = len(df_train.iloc[0]['mfcc'])  # Feature dimension for mfcc\n",
    "    num_classes = len(df_train.iloc[0]['label']) # Number of phoneme classes\n",
    "    dtype = torch.float32\n",
    "    x = torch.zeros((minibatch_size, input_size), dtype=dtype)  # minibatch size 64, feature dimension 20\n",
    "    model = DNN_FC(input_size, num_classes)\n",
    "    scores = model(x)\n",
    "    print(scores.size())  # you should see [minibatch_size, num_classes]\n",
    "test_DNN_FC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(df_train['mfcc'][0])\n",
    "num_classes = len(df_train['label'][0])\n",
    "learning_rate = 1e-2\n",
    "model = DNN_FC(input_size, num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate) \n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "train_loss_lst, val_loss_lst = train(model, optimizer, scheduler, epochs = 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_loss_lst)\n",
    "print(val_loss_lst) \n",
    "\n",
    "# Plot the accuracy values\n",
    "plt.plot(val_loss_lst[2:], label='Validation Loss')\n",
    "plt.plot(train_loss_lst[2:], label='Training Loss')\n",
    "\n",
    "# Add labels and title to the plot\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Save the model parameters\n",
    "#torch.save(model.state_dict(), 'model_parameters.pth')\n",
    "\n",
    "# To load the model parameters, use the following procedure: \n",
    "# Create an instance of the neural network\n",
    "#new_model = DNN_FC(input_size, num_classes)\n",
    "# Load the saved model parameters\n",
    "#model_parameters = torch.load('model_parameters.pth')\n",
    "# Assign the loaded parameters to the model\n",
    "#new_model.load_state_dict(model_parameters) \n",
    "\n",
    "check_accuracy(loader_val, model) \n",
    "check_accuracy(loader_test, model) \n",
    "loader_train_fortest = DataLoader(dataset_train, batch_size=1,\n",
    "                                  sampler=sampler.SequentialSampler(range(len(df_train))))\n",
    "\n",
    "check_accuracy(loader_train_fortest, model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy(loader_val, model)\n",
    "check_accuracy(loader_test, model) \n",
    "check_accuracy(loader_train, model) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_probabilities(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print('Getting estimated probabilities on validation set')\n",
    "    else:\n",
    "        print('Getting estimated probabilities on test set') \n",
    "    model.eval()  # set model to evaluation mode\n",
    "    probabilities_dict = {} \n",
    "    batch_size = loader.batch_size\n",
    "    with torch.no_grad():\n",
    "        for idx, (x, y) in enumerate(loader):\n",
    "            y = flatten(y) # Flatten y to convert dimension from (Nx1) to (N,)\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=dtype) \n",
    "            scores = model(x) \n",
    "            probabilities = torch.softmax(scores, dim=1) \n",
    "            \n",
    "            # Save the probabilities with the corresponding row index\n",
    "            for i in range(len(probabilities)):\n",
    "                probabilities_dict[idx * batch_size + i] = probabilities[i].numpy()\n",
    "    \n",
    "    return probabilities_dict\n",
    "\n",
    "\n",
    "def find_emission(loader, model, scale = False):\n",
    "    '''\n",
    "    Find emission probabilities for a given data loader and model.\n",
    "    Consider changing this function if it takes too long. Currently: O(n) \n",
    "    Args:\n",
    "        loader: torch Data loader\n",
    "        model: torch DNN model\n",
    "        scale: Boolean: Set true to scale the output probability of DNN.\n",
    "    Returns:\n",
    "        emission_df: Dataframe for emission probabilities.\n",
    "    '''\n",
    "    # Get the inferred probabilities for each class (12 states, background and silence)\n",
    "    probabilities_dict = infer_probabilities(loader, model) \n",
    "    emission = probabilities_dict\n",
    "    # Get the prior vector and the transition probabilities. We don't need the transition probabilities.\n",
    "    prior_vector, _ = get_prob.main(rerun=False) \n",
    "\n",
    "    # For each key=row_idx and val=prob_array, convert the inferred probabilities into emission.\n",
    "    for key, val in emission.items():\n",
    "        # Slice val to exclude the probabilities for background and silence.\n",
    "        if scale == True:\n",
    "            log_prob = np.where(val > 0, np.log(val), -np.inf)   # Get the log probabilities. \n",
    "            log_prob = log_prob[:-2]  # Exclude the background and silence in the emission probability calculation. \n",
    "            emission[key] = [log_prob-prior_vector]  # Divide by prior vector in the log space. \n",
    "        else:\n",
    "            emission[key] = [val] \n",
    "\n",
    "    emission_df = pd.DataFrame.from_dict(emission, orient='index', columns=['Emission']) \n",
    "    return emission_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_vector, _ = get_prob.main(rerun=True)\n",
    "estimate_prob = infer_probabilities(loader_test, model)\n",
    "emission_data = find_emission(loader_test, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_emission(file_path_wav: str, file_path_phn: str):\n",
    "    '''\n",
    "    Given the path of a file, get the emission probabilities.\n",
    "    Args:\n",
    "        file_path: Path of the audio file as a string.\n",
    "    Returns:\n",
    "        emit: pd.dataframe\n",
    "            Emission probabilities for each frame in the audio file.\n",
    "    '''\n",
    "    df_test = mfcc_label.prepare_data(file_path_phn,file_path_wav)\n",
    "    column_str_to_numpy(df_test, 'mfcc')\n",
    "    column_str_to_numpy(df_test, 'label')\n",
    "    # Convert dataframe into a loader so that torch can work with.\n",
    "    dataset_test = CustomDataset(df_test,train=False)\n",
    "    loader_test = DataLoader(dataset_test, batch_size=1,\n",
    "                        sampler=sampler.SequentialSampler(range(len(df_test))))\n",
    "\n",
    "    emission_data = find_emission(loader_test, model)\n",
    "    return emission_data\n",
    "    \n",
    "\n",
    "path_to_emission('timit/data/TRAIN/DR4/MDCD0/SX425.WAV','timit/data/TRAIN/DR4/MDCD0/SX425.PHN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emission_all_paths(path_type: str = 'test'):\n",
    "    paths = load('processed_data/train_test_dataset_never.joblib')[path_type]\n",
    "    data = {}\n",
    "    for i in range(len(paths)):\n",
    "        file_path_wav, file_path_phn, file_path_word = paths[i]\n",
    "        emission_data = path_to_emission(file_path_wav, file_path_phn)\n",
    "        data[(file_path_wav, file_path_phn, file_path_word)] = emission_data\n",
    "\n",
    "    return data\n",
    "data = get_emission_all_paths() \n",
    "\n",
    "from joblib import dump\n",
    "dump(data, \"processed_data/test_data_for_hmm.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F  \n",
    "from DNN_utils import (flatten) \n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "# Import helper functions.\n",
    "import mfcc_label \n",
    "import get_prob\n",
    "\n",
    "\n",
    "df_train = pd.read_csv('processed_data/dnn_never_train.csv')\n",
    "\n",
    "if isinstance(df_train.iloc[0]['mfcc'], str):\n",
    "    df_train['mfcc'] = df_train['mfcc'].apply(lambda x: np.fromstring(x[1:-1], sep=' '))\n",
    "if isinstance(df_train.iloc[0]['label'], str):\n",
    "    df_train['label'] = df_train['label'].apply(lambda x: np.fromstring(x[1:-1], sep=' '))\n",
    "\n",
    "\n",
    "# Configurations \n",
    "NUM_TRAIN = int(0.8*len(df_train)) # Number of training examples for splitting training and validation datasets. \n",
    "NUM_ROWS = len(df_train)\n",
    "device = 'cpu'\n",
    "dtype = torch.float32\n",
    "print_every = 50\n",
    "\n",
    "# DNN Architecture Hyperparameters\n",
    "minibatch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('timit/data/TRAIN/DR3/FLTM0/SX440.WAV',\n",
       "  'timit/data/TRAIN/DR3/FLTM0/SX440.PHN',\n",
       "  'timit/data/TRAIN/DR3/FLTM0/SX440.WRD'),\n",
       " ('timit/data/TRAIN/DR1/FDAW0/SX146.WAV',\n",
       "  'timit/data/TRAIN/DR1/FDAW0/SX146.PHN',\n",
       "  'timit/data/TRAIN/DR1/FDAW0/SX146.WRD'),\n",
       " ('timit/data/TRAIN/DR3/MDBB1/SX376.WAV',\n",
       "  'timit/data/TRAIN/DR3/MDBB1/SX376.PHN',\n",
       "  'timit/data/TRAIN/DR3/MDBB1/SX376.WRD'),\n",
       " ('timit/data/TRAIN/DR8/FMBG0/SX440.WAV',\n",
       "  'timit/data/TRAIN/DR8/FMBG0/SX440.PHN',\n",
       "  'timit/data/TRAIN/DR8/FMBG0/SX440.WRD'),\n",
       " ('timit/data/TRAIN/DR2/FMMH0/SI907.WAV',\n",
       "  'timit/data/TRAIN/DR2/FMMH0/SI907.PHN',\n",
       "  'timit/data/TRAIN/DR2/FMMH0/SI907.WRD'),\n",
       " ('timit/data/TRAIN/DR3/FEME0/SX425.WAV',\n",
       "  'timit/data/TRAIN/DR3/FEME0/SX425.PHN',\n",
       "  'timit/data/TRAIN/DR3/FEME0/SX425.WRD'),\n",
       " ('timit/data/TRAIN/DR5/MCLM0/SX376.WAV',\n",
       "  'timit/data/TRAIN/DR5/MCLM0/SX376.PHN',\n",
       "  'timit/data/TRAIN/DR5/MCLM0/SX376.WRD'),\n",
       " ('timit/data/TRAIN/DR7/MDLC1/SI2065.WAV',\n",
       "  'timit/data/TRAIN/DR7/MDLC1/SI2065.PHN',\n",
       "  'timit/data/TRAIN/DR7/MDLC1/SI2065.WRD'),\n",
       " ('timit/data/TRAIN/DR7/FKDE0/SX61.WAV',\n",
       "  'timit/data/TRAIN/DR7/FKDE0/SX61.PHN',\n",
       "  'timit/data/TRAIN/DR7/FKDE0/SX61.WRD'),\n",
       " ('timit/data/TRAIN/DR4/MCSS0/SX210.WAV',\n",
       "  'timit/data/TRAIN/DR4/MCSS0/SX210.PHN',\n",
       "  'timit/data/TRAIN/DR4/MCSS0/SX210.WRD'),\n",
       " ('timit/data/TRAIN/DR7/MSES0/SI2216.WAV',\n",
       "  'timit/data/TRAIN/DR7/MSES0/SI2216.PHN',\n",
       "  'timit/data/TRAIN/DR7/MSES0/SI2216.WRD'),\n",
       " ('timit/data/TRAIN/DR8/FBCG1/SI1612.WAV',\n",
       "  'timit/data/TRAIN/DR8/FBCG1/SI1612.PHN',\n",
       "  'timit/data/TRAIN/DR8/FBCG1/SI1612.WRD'),\n",
       " ('timit/data/TRAIN/DR2/MRMS0/SX210.WAV',\n",
       "  'timit/data/TRAIN/DR2/MRMS0/SX210.PHN',\n",
       "  'timit/data/TRAIN/DR2/MRMS0/SX210.WRD'),\n",
       " ('timit/data/TRAIN/DR1/MTPF0/SX425.WAV',\n",
       "  'timit/data/TRAIN/DR1/MTPF0/SX425.PHN',\n",
       "  'timit/data/TRAIN/DR1/MTPF0/SX425.WRD'),\n",
       " ('timit/data/TRAIN/DR7/MREM0/SX61.WAV',\n",
       "  'timit/data/TRAIN/DR7/MREM0/SX61.PHN',\n",
       "  'timit/data/TRAIN/DR7/MREM0/SX61.WRD'),\n",
       " ('timit/data/TRAIN/DR7/MDPB0/SX146.WAV',\n",
       "  'timit/data/TRAIN/DR7/MDPB0/SX146.PHN',\n",
       "  'timit/data/TRAIN/DR7/MDPB0/SX146.WRD'),\n",
       " ('timit/data/TRAIN/DR5/MTAT0/SX210.WAV',\n",
       "  'timit/data/TRAIN/DR5/MTAT0/SX210.PHN',\n",
       "  'timit/data/TRAIN/DR5/MTAT0/SX210.WRD'),\n",
       " ('timit/data/TRAIN/DR5/FKKH0/SX210.WAV',\n",
       "  'timit/data/TRAIN/DR5/FKKH0/SX210.PHN',\n",
       "  'timit/data/TRAIN/DR5/FKKH0/SX210.WRD'),\n",
       " ('timit/data/TRAIN/DR3/MRJB1/SX210.WAV',\n",
       "  'timit/data/TRAIN/DR3/MRJB1/SX210.PHN',\n",
       "  'timit/data/TRAIN/DR3/MRJB1/SX210.WRD'),\n",
       " ('timit/data/TRAIN/DR4/MJLS0/SX376.WAV',\n",
       "  'timit/data/TRAIN/DR4/MJLS0/SX376.PHN',\n",
       "  'timit/data/TRAIN/DR4/MJLS0/SX376.WRD'),\n",
       " ('timit/data/TRAIN/DR3/MTJM0/SX146.WAV',\n",
       "  'timit/data/TRAIN/DR3/MTJM0/SX146.PHN',\n",
       "  'timit/data/TRAIN/DR3/MTJM0/SX146.WRD'),\n",
       " ('timit/data/TRAIN/DR3/MDWM0/SX376.WAV',\n",
       "  'timit/data/TRAIN/DR3/MDWM0/SX376.PHN',\n",
       "  'timit/data/TRAIN/DR3/MDWM0/SX376.WRD'),\n",
       " ('timit/data/TRAIN/DR8/MRDM0/SX425.WAV',\n",
       "  'timit/data/TRAIN/DR8/MRDM0/SX425.PHN',\n",
       "  'timit/data/TRAIN/DR8/MRDM0/SX425.WRD'),\n",
       " ('timit/data/TEST/DR3/MCSH0/SI2179.WAV',\n",
       "  'timit/data/TEST/DR3/MCSH0/SI2179.PHN',\n",
       "  'timit/data/TEST/DR3/MCSH0/SI2179.WRD'),\n",
       " ('timit/data/TRAIN/DR1/MJEB1/SI2097.WAV',\n",
       "  'timit/data/TRAIN/DR1/MJEB1/SI2097.PHN',\n",
       "  'timit/data/TRAIN/DR1/MJEB1/SI2097.WRD'),\n",
       " ('timit/data/TRAIN/DR7/FLEH0/SX61.WAV',\n",
       "  'timit/data/TRAIN/DR7/FLEH0/SX61.PHN',\n",
       "  'timit/data/TRAIN/DR7/FLEH0/SX61.WRD'),\n",
       " ('timit/data/TRAIN/DR4/MARW0/SX376.WAV',\n",
       "  'timit/data/TRAIN/DR4/MARW0/SX376.PHN',\n",
       "  'timit/data/TRAIN/DR4/MARW0/SX376.WRD'),\n",
       " ('timit/data/TRAIN/DR3/FJLR0/SX61.WAV',\n",
       "  'timit/data/TRAIN/DR3/FJLR0/SX61.PHN',\n",
       "  'timit/data/TRAIN/DR3/FJLR0/SX61.WRD'),\n",
       " ('timit/data/TRAIN/DR3/MDHS0/SI2160.WAV',\n",
       "  'timit/data/TRAIN/DR3/MDHS0/SI2160.PHN',\n",
       "  'timit/data/TRAIN/DR3/MDHS0/SI2160.WRD')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import load\n",
    "load('processed_data/train_test_dataset_never.joblib')['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 11])\n"
     ]
    }
   ],
   "source": [
    "class DNN_FC(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "        # assign layer objects to class attributes\n",
    "        # We may write a loop if we use the same activation function for all layers.\n",
    "        self.fc1 = nn.Linear(input_size, input_size)\n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        self.fc2 = nn.Linear(input_size, input_size)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight) \n",
    "        self.fc3 = nn.Linear(input_size, input_size)\n",
    "        nn.init.kaiming_normal_(self.fc3.weight)\n",
    "        self.fc4 = nn.Linear(input_size, input_size)\n",
    "        nn.init.kaiming_normal_(self.fc4.weight)\n",
    "        self.fc5 = nn.Linear(input_size, num_classes)\n",
    "        nn.init.kaiming_normal_(self.fc5.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_temp = x\n",
    "        x_temp = flatten(x_temp)\n",
    "        x_temp = F.relu(self.fc1(x_temp))\n",
    "        x_temp = F.relu(self.fc2(x_temp))\n",
    "        x_temp = F.relu(self.fc3(x_temp))\n",
    "        x_temp = F.relu(self.fc4(x_temp))\n",
    "        scores = self.fc5(x_temp)\n",
    "        return scores\n",
    "\n",
    "\n",
    "def test_DNN_FC():\n",
    "    input_size = 20  # Feature dimension for mfcc\n",
    "    num_classes = 11 # Number of phoneme classes\n",
    "    dtype = torch.float32\n",
    "    x = torch.zeros((minibatch_size, input_size), dtype=dtype)  # minibatch size 64, feature dimension 20\n",
    "    model = DNN_FC(input_size, num_classes)\n",
    "    scores = model(x)\n",
    "    print(scores.size())  # you should see [minibatch_size, num_classes]\n",
    "test_DNN_FC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mfcc</th>\n",
       "      <th>label</th>\n",
       "      <th>state_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-730.505798, 45.0450668, -24.2304726, -16.595...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>{'h#': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-714.74677, 51.683174, -20.392345, -13.274165...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>{'h#': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-720.12866, 46.353207, -18.542915, -15.895822...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>{'h#': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-726.85284, 38.16919, -21.354225, -16.98072, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>{'h#': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-733.83295, 39.958935, -22.630146, -19.272156...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>{'h#': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15488</th>\n",
       "      <td>[-773.1762, 4.0019016, 2.5322413, 2.8646927, 3...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>{'h#': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15489</th>\n",
       "      <td>[-773.2592, 4.076532, 2.7111504, 3.3019888, 4....</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>{'h#': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15490</th>\n",
       "      <td>[-774.35565, 3.5023093, 3.4348507, 3.9783638, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>{'h#': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15491</th>\n",
       "      <td>[-774.29865, 4.247159, 3.8669136, 3.643158, 3....</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>{'h#': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15492</th>\n",
       "      <td>[-774.073, 4.307059, 4.27923, 4.0476756, 3.220...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>{'h#': 1.0}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15493 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    mfcc  \\\n",
       "0      [-730.505798, 45.0450668, -24.2304726, -16.595...   \n",
       "1      [-714.74677, 51.683174, -20.392345, -13.274165...   \n",
       "2      [-720.12866, 46.353207, -18.542915, -15.895822...   \n",
       "3      [-726.85284, 38.16919, -21.354225, -16.98072, ...   \n",
       "4      [-733.83295, 39.958935, -22.630146, -19.272156...   \n",
       "...                                                  ...   \n",
       "15488  [-773.1762, 4.0019016, 2.5322413, 2.8646927, 3...   \n",
       "15489  [-773.2592, 4.076532, 2.7111504, 3.3019888, 4....   \n",
       "15490  [-774.35565, 3.5023093, 3.4348507, 3.9783638, ...   \n",
       "15491  [-774.29865, 4.247159, 3.8669136, 3.643158, 3....   \n",
       "15492  [-774.073, 4.307059, 4.27923, 4.0476756, 3.220...   \n",
       "\n",
       "                                                   label state_weights  \n",
       "0      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   {'h#': 1.0}  \n",
       "1      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   {'h#': 1.0}  \n",
       "2      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   {'h#': 1.0}  \n",
       "3      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   {'h#': 1.0}  \n",
       "4      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   {'h#': 1.0}  \n",
       "...                                                  ...           ...  \n",
       "15488  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   {'h#': 1.0}  \n",
       "15489  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   {'h#': 1.0}  \n",
       "15490  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   {'h#': 1.0}  \n",
       "15491  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   {'h#': 1.0}  \n",
       "15492  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   {'h#': 1.0}  \n",
       "\n",
       "[15493 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(type(df_train.iloc[0]['label']))\n",
    "# If the type is str, convert it\n",
    "\n",
    "display(df_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#mfcc = df_train.iloc[0:5]['mfcc']\n",
    "#mfcc = np.vstack(mfcc)\n",
    "#mfcc \n",
    "\n",
    "\n",
    "labels = df_train.iloc[0:5]['label']\n",
    "labels = np.vstack(labels)\n",
    "display(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-7.3051e+02,  4.5045e+01, -2.4230e+01, -1.6596e+01,  7.8798e+00,\n",
       "           6.0388e-01, -1.2444e+01,  1.5479e+00,  1.8096e+01,  1.0397e+01,\n",
       "           9.5008e-01,  6.5883e+00,  1.3456e+01,  4.3042e+00, -9.5567e+00,\n",
       "          -5.4092e+00,  2.0956e+00,  1.1521e+00,  4.0445e+00,  4.9519e+00],\n",
       "         [-7.1475e+02,  5.1683e+01, -2.0392e+01, -1.3274e+01, -3.0518e+00,\n",
       "          -3.8946e+00, -7.8653e+00,  4.2943e+00,  1.9027e+01,  5.3817e+00,\n",
       "          -7.3529e+00,  3.3265e+00,  9.8732e+00,  4.8012e+00, -1.0586e+01,\n",
       "          -8.3340e+00,  1.0399e+01,  1.2690e+01,  1.1761e+01,  8.7251e+00],\n",
       "         [-7.2013e+02,  4.6353e+01, -1.8543e+01, -1.5896e+01, -1.6697e+01,\n",
       "          -6.0928e+00, -1.3419e+00,  5.4095e+00,  2.1859e+01,  5.5194e+00,\n",
       "          -1.4134e+01,  5.9534e-01,  9.5893e+00,  5.5199e+00, -6.0089e+00,\n",
       "          -5.3524e+00,  1.0020e+01,  1.4292e+01,  1.3084e+01,  5.8263e+00],\n",
       "         [-7.2685e+02,  3.8169e+01, -2.1354e+01, -1.6981e+01, -1.2307e+01,\n",
       "          -1.3834e+00, -1.4019e+00,  6.3001e+00,  2.7746e+01,  1.1269e+01,\n",
       "          -1.0486e+01,  4.7258e+00,  9.7506e+00,  5.9056e+00, -5.1041e+00,\n",
       "          -2.1838e+00,  6.9739e+00,  1.0429e+01,  1.2742e+01,  9.5931e+00],\n",
       "         [-7.3383e+02,  3.9959e+01, -2.2630e+01, -1.9272e+01, -8.4695e-01,\n",
       "           6.8247e+00,  1.1924e+00,  1.3461e+01,  2.9216e+01,  3.6541e+00,\n",
       "          -1.4534e+01,  2.3607e+00,  8.8360e+00,  4.5712e-01, -7.8657e+00,\n",
       "          -2.6887e+00,  6.6100e+00,  7.1146e+00,  7.7449e+00,  8.8961e+00]]),\n",
       " tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert dataset into a format that torch can read.\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None, train=True):\n",
    "        self.df = dataframe\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the rows that are selected by the idx.\n",
    "        mfcc = self.df.iloc[idx]['mfcc']\n",
    "        label = self.df.iloc[idx]['label']\n",
    "\n",
    "        # Stack the rows for mfcc and label.\n",
    "        # Stack a list of (1,n) dimensional np.ndarrays into (m,n) dimensional np.ndarray. \n",
    "        mfcc = np.vstack(mfcc)\n",
    "        label = np.vstack(label)\n",
    "\n",
    "        # Convert 2 dimensional np.ndarrays into torch tensors.\n",
    "        mfcc = torch.tensor(mfcc, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "        if self.transform:\n",
    "            mfcc = self.transform(mfcc)\n",
    "\n",
    "        return mfcc, label\n",
    "\n",
    "# Create an instance of your dataset with your DataFrame\n",
    "dataset_train = CustomDataset(df_train,train=True)  # Assuming df is your pandas DataFrame\n",
    "\n",
    "# Create the DataLoader to handle batching\n",
    "loader_train = DataLoader(dataset_train, batch_size=minibatch_size,\n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "# Create the DataLoader to handle batching\n",
    "loader_val = DataLoader(dataset_train, batch_size=1,\n",
    "                        sampler=sampler.SequentialSampler(range(NUM_TRAIN, NUM_ROWS)))\n",
    "display(dataset_train.__getitem__(range(5))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Features (MFCCs) size: torch.Size([512, 20, 1])\n",
      "Labels size: torch.Size([512, 14, 1])\n",
      "\n",
      "\n",
      "Batch 2\n",
      "Features (MFCCs) size: torch.Size([512, 20, 1])\n",
      "Labels size: torch.Size([512, 14, 1])\n",
      "\n",
      "\n",
      "Batch 3\n",
      "Features (MFCCs) size: torch.Size([512, 20, 1])\n",
      "Labels size: torch.Size([512, 14, 1])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example code to print the contents of the first few batches in loader_train\n",
    "for i, (inputs, labels) in enumerate(loader_train):\n",
    "    print(f\"Batch {i + 1}\")\n",
    "    print(\"Features (MFCCs) size:\", inputs.size())\n",
    "    print(\"Labels size:\", labels.size())\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Optional: Stop after a few batches to avoid flooding the output\n",
    "    if i == 2:  # Adjust this number based on how many batches you want to see\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            y = flatten(y) # Flatten y to convert dimension from (Nx1) to (N,)\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=dtype) \n",
    "            scores = model(x) \n",
    "            _, preds = scores.max(1) \n",
    "            true_class = y.argmax(dim=1) # True class is the one that has the highest probability in the data.\n",
    "            num_correct += (preds == true_class).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "    return acc \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "dtype = torch.float32\n",
    "\n",
    "def train(model, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    Train the model using the PyTorch Module API.\n",
    "\n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "\n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    accuracy_val_lst = []\n",
    "    accuracy_cal_max = 0\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            y = flatten(y) # Flatten y to convert the dimension from (Nx1) to (N,)\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=dtype)\n",
    "\n",
    "            scores = model(x)\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "            loss = criterion(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "            \n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                print()\n",
    "                accuracy_val = check_accuracy(loader_val, model)\n",
    "                if accuracy_val > accuracy_cal_max:\n",
    "                    accuracy_cal_max = accuracy_val\n",
    "                    model_params = model.state_dict()\n",
    "                accuracy_val_lst.append((t,accuracy_val))\n",
    "        \n",
    "    print('Training is complete. Accuracies on the validation set are:') \n",
    "    print(accuracy_val_lst)\n",
    "    return \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 39.5990\n",
      "\n",
      "Checking accuracy on validation set\n",
      "Got 0 / 3099 correct (0.00)\n",
      "Iteration 0, loss = 0.1602\n",
      "\n",
      "Checking accuracy on validation set\n",
      "Got 1910 / 3099 correct (61.63)\n",
      "Iteration 0, loss = 0.0933\n",
      "\n",
      "Checking accuracy on validation set\n",
      "Got 1958 / 3099 correct (63.18)\n",
      "Iteration 0, loss = 0.0768\n",
      "\n",
      "Checking accuracy on validation set\n",
      "Got 2005 / 3099 correct (64.70)\n",
      "Iteration 0, loss = 0.0695\n",
      "\n",
      "Checking accuracy on validation set\n",
      "Got 2027 / 3099 correct (65.41)\n",
      "Iteration 0, loss = 0.0773\n",
      "\n",
      "Checking accuracy on validation set\n",
      "Got 2010 / 3099 correct (64.86)\n",
      "Iteration 0, loss = 0.0785\n",
      "\n",
      "Checking accuracy on validation set\n",
      "Got 2033 / 3099 correct (65.60)\n",
      "Iteration 0, loss = 0.0734\n",
      "\n",
      "Checking accuracy on validation set\n",
      "Got 2029 / 3099 correct (65.47)\n",
      "Iteration 0, loss = 0.0751\n",
      "\n",
      "Checking accuracy on validation set\n",
      "Got 2056 / 3099 correct (66.34)\n",
      "Iteration 0, loss = 0.0722\n",
      "\n",
      "Checking accuracy on validation set\n",
      "Got 2047 / 3099 correct (66.05)\n",
      "Training is complete. Accuracies on the validation set are:\n",
      "[(0, 0.0), (0, 0.6163278476928041), (0, 0.6318167150693772), (0, 0.6469828977089384), (0, 0.654081961923201), (0, 0.648596321393998), (0, 0.6560180703452727), (0, 0.6547273313972249), (0, 0.6634398192965473), (0, 0.6605356566634398)]\n"
     ]
    }
   ],
   "source": [
    "input_size = len(df_train['mfcc'][0])\n",
    "num_classes = len(df_train['label'][0])\n",
    "learning_rate = 1e-2\n",
    "model = DNN_FC(input_size, num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "train(model, optimizer,epochs = 10) \n",
    "\n",
    "# Accuracy on the validation set: 72.57%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_probabilities(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print('Getting estimated probabilities on validation set')\n",
    "    else:\n",
    "        print('Getting estimated probabilities on test set') \n",
    "    model.eval()  # set model to evaluation mode\n",
    "    probabilities_dict = {} \n",
    "    with torch.no_grad():\n",
    "        for idx, (x, y) in enumerate(loader):\n",
    "            y = flatten(y) # Flatten y to convert dimension from (Nx1) to (N,)\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=dtype) \n",
    "            scores = model(x) \n",
    "            probabilities = torch.softmax(scores, dim=1) \n",
    "            \n",
    "            # Save the probabilities with the corresponding row index\n",
    "            for i in range(len(probabilities)):\n",
    "                probabilities_dict[idx * minibatch_size + i] = probabilities[i].numpy()\n",
    "    \n",
    "    return probabilities_dict\n",
    "\n",
    "\n",
    "def find_emission(loader, model):\n",
    "    '''\n",
    "    Find emission probabilities for a given data loader and model.\n",
    "    Consider changing this function if it takes too long. Currently: O(n)\n",
    "    '''\n",
    "    # Get the inferred probabilities for each class (12 states, background and silence)\n",
    "    probabilities_dict = infer_probabilities(loader, model) \n",
    "\n",
    "    # Get the prior vector and the transition probabilities. We don't need the transition probabilities.\n",
    "    prior_vector, _ = get_prob.main(log_space=False)\n",
    "\n",
    "    # For each key=row_idx and val=prob_array, convert the inferred probabilities into emission.\n",
    "    for key, val in probabilities_dict.items():\n",
    "        # Slice val to exclude the probabilities for background and silence.\n",
    "        probabilities_dict[key] = probabilities_dict[key][:-2]/prior_vector  \n",
    "    \n",
    "    return probabilities_dict\n",
    "\n",
    "\n",
    "\n",
    "# File id: Keep path of the audio file, and probabilities with size (num_frames, num_classes).  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting estimated probabilities on validation set\n",
      "Getting estimated probabilities on validation set\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'log_prior'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'log_prior'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m estimate_prob \u001b[38;5;241m=\u001b[39m infer_probabilities(loader_val, model)\n\u001b[0;32m----> 2\u001b[0m emission_data \u001b[38;5;241m=\u001b[39m \u001b[43mfind_emission\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m prior_vector, _ \u001b[38;5;241m=\u001b[39m get_prob\u001b[38;5;241m.\u001b[39mmain(log_space\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(estimate_prob[\u001b[38;5;241m0\u001b[39m][:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m/\u001b[39mprior_vector)\n",
      "Cell \u001b[0;32mIn[20], line 32\u001b[0m, in \u001b[0;36mfind_emission\u001b[0;34m(loader, model)\u001b[0m\n\u001b[1;32m     29\u001b[0m probabilities_dict \u001b[38;5;241m=\u001b[39m infer_probabilities(loader, model) \n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Get the prior vector and the transition probabilities. We don't need the transition probabilities.\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m prior_vector, _ \u001b[38;5;241m=\u001b[39m \u001b[43mget_prob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# For each key=row_idx and val=prob_array, convert the inferred probabilities into emission.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m probabilities_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Slice val to exclude the probabilities for background and silence.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-emin.ozyoruk@chicagobooth.edu/My Drive/Chicago/Academic/Booth 4.3/Probabilistic Graphical Models/Project/Codes/keyword-spotting/get_prob.py:328\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(keyword, dataset_type, phoneme_list, log_space, rerun)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;124;03mmain function to get the prior vector and transition matrix\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    324\u001b[0m df_prior, df_transition \u001b[38;5;241m=\u001b[39m get_prior_transition(\n\u001b[1;32m    325\u001b[0m     keyword, dataset_type, phoneme_list, log_space, rerun\n\u001b[1;32m    326\u001b[0m )\n\u001b[0;32m--> 328\u001b[0m prior_vector \u001b[38;5;241m=\u001b[39m \u001b[43mget_prior_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_prior\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m transition_matrix \u001b[38;5;241m=\u001b[39m trainsition_matrix(df_transition)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prior_vector, transition_matrix\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-emin.ozyoruk@chicagobooth.edu/My Drive/Chicago/Academic/Booth 4.3/Probabilistic Graphical Models/Project/Codes/keyword-spotting/get_prob.py:281\u001b[0m, in \u001b[0;36mget_prior_vector\u001b[0;34m(prior_df, drop_background)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03mGet the prior probability vector from the prior DataFrame.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m drop_background:\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mprior_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlog_prior\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(prior_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_prior\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'log_prior'"
     ]
    }
   ],
   "source": [
    "estimate_prob = infer_probabilities(loader_val, model)\n",
    "emission_data = find_emission(loader_val, model)\n",
    "\n",
    "prior_vector, _ = get_prob.main(log_space=False)\n",
    "\n",
    "\n",
    "\n",
    "print(estimate_prob[0][:-2]/prior_vector)\n",
    "print(emission_data[0])\n",
    "print('THERE IS A NUMERICAL ERROR. CHECK THIS.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strategies for training DNN:\n",
    "\n",
    "Upsampling strategies:\n",
    "\n",
    "1- Resample the minority classes such that in half of the data, one of the first 12 classes have positive probabilities (the other half has zero probabilities for the first 12 classes).\n",
    "\n",
    "2- Resample the minority classes such that each class has the same number of observations as class 14 (#b). \n",
    "\n",
    "3- Resample the minority classes such that each class has the same number of observations as class 13 (h#).\n",
    "\n",
    "Changing the objective function:\n",
    "\n",
    "1- KL divergence\n",
    "\n",
    "2- CrossEntropyLoss\n",
    "\n",
    "3- CrossEntropyLoss with weights given to 12 classes.\n",
    "\n",
    "3- Train with single class labels (rather than vectors).\n",
    "\n",
    "Changing the architecture:\n",
    "\n",
    "1- More layers or less layers.\n",
    "\n",
    "2- Relu or sigmoid activation functions.\n",
    "\n",
    "Chaning the data:\n",
    "\n",
    "1- Aggregate class labes (1 class per phoneme)\n",
    "\n",
    "Changing HMM\n",
    "\n",
    "1- Instead of highest probability path, calculate v_C(T) \n",
    "\n",
    "2- Try different window length (instead of 60 frames). Calculate v_C(T)/T to get a normalized score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F  \n",
    "import DNN_utils\n",
    "from DNN_utils import *\n",
    "#from DNN_utils import (flatten, column_str_to_numpy, check_accuracy, check_loss, check_accuracy_12_classes)\n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from joblib import load \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import helper functions.\n",
    "import mfcc_label \n",
    "import get_prob\n",
    "\n",
    "# Read the data\n",
    "df_train_val = pd.read_csv('processed_data/dnn_never_train.csv')\n",
    "df_test = pd.read_csv('processed_data/dnn_never_test.csv')\n",
    "\n",
    "# Some columns are recorded as string although they are arrays.\n",
    "column_str_to_numpy(df_train_val, 'mfcc')\n",
    "column_str_to_numpy(df_train_val, 'label')\n",
    "column_str_to_numpy(df_test, 'mfcc')\n",
    "column_str_to_numpy(df_test, 'label')\n",
    "\n",
    "#Split the train set into train and validation sets.\n",
    "df_train_pre, df_val = train_test_split(df_train_val, test_size=0.2, random_state=42)\n",
    "df_train_pre.reset_index(drop=True, inplace=True)\n",
    "df_val.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Create a single class label (type: int) which is the highest probability class in the label vector (type: 14x1 array).\n",
    "df_train_pre['single_class_label'] = df_train_pre['label'].apply(lambda x: np.argmax(x))\n",
    "df_val['single_class_label'] = df_val['label'].apply(lambda x: np.argmax(x))\n",
    "df_test['single_class_label'] = df_test['label'].apply(lambda x: np.argmax(x))\n",
    "\n",
    "# Configurations \n",
    "#NUM_TRAIN = int(0.8*len(df_train_pre)) # Number of training examples for splitting training and validation datasets. \n",
    "#NUM_ROWS = len(df_train_pre)\n",
    "device = DNN_utils.device\n",
    "dtype = DNN_utils.dtype\n",
    "\n",
    "# DNN Architecture Hyperparameters\n",
    "minibatch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15493\n",
      "12394\n",
      "3099\n",
      "17249\n"
     ]
    }
   ],
   "source": [
    "print(len(df_train_val))\n",
    "print(len(df_train_pre))\n",
    "print(len(df_val)) \n",
    "print(len(df_test)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\npd.set_option('display.max_colwidth', None)\\n\\n# Upsample from observations that give positive probability on one of the 12 classes that correspond to 'never'. \\ndef upsample_minority(df, mask):\\n    from sklearn.utils import resample\\n    df_minority = df[mask]\\n    df_majority = df[~mask]\\n\\n    df_minority_upsampled = resample(df_minority,\\n                                    replace=True,     # sample with replacement\\n                                    n_samples=len(df_majority),    # to match majority class\\n                                    random_state=42) # reproducible results\\n\\n    df_upsampled = pd.concat([df_majority, df_minority_upsampled])\\n    return df_upsampled\\n\\nmask = df_train_pre['label'].apply(lambda x: any(elem > 0 for elem in x[:12]))\\n#mask = df_train_pre['label'].apply(lambda x: all(elem == 0 for elem in x[12:]))\\ndf_train = upsample_minority(df_train_pre, mask)\\n\\nprint('Before upsampling:')\\ndisplay(df_train_pre['label'].apply(lambda x: any(elem > 0 for elem in x[:12])).value_counts()) \\n\\n# Display new class counts\\nprint('After upsampling:')\\ndisplay(df_train['label'].apply(lambda x: any(elem > 0 for elem in x[:12])).value_counts()) \\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Upsample from observations that give positive probability on one of the 12 classes that correspond to 'never'. \n",
    "def upsample_minority(df, mask):\n",
    "    from sklearn.utils import resample\n",
    "    df_minority = df[mask]\n",
    "    df_majority = df[~mask]\n",
    "\n",
    "    df_minority_upsampled = resample(df_minority,\n",
    "                                    replace=True,     # sample with replacement\n",
    "                                    n_samples=len(df_majority),    # to match majority class\n",
    "                                    random_state=42) # reproducible results\n",
    "\n",
    "    df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "    return df_upsampled\n",
    "\n",
    "mask = df_train_pre['label'].apply(lambda x: any(elem > 0 for elem in x[:12]))\n",
    "#mask = df_train_pre['label'].apply(lambda x: all(elem == 0 for elem in x[12:]))\n",
    "df_train = upsample_minority(df_train_pre, mask)\n",
    "\n",
    "print('Before upsampling:')\n",
    "display(df_train_pre['label'].apply(lambda x: any(elem > 0 for elem in x[:12])).value_counts()) \n",
    "\n",
    "# Display new class counts\n",
    "print('After upsampling:')\n",
    "display(df_train['label'].apply(lambda x: any(elem > 0 for elem in x[:12])).value_counts()) \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mfcc</th>\n",
       "      <th>label</th>\n",
       "      <th>state_weights</th>\n",
       "      <th>single_class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-705.99225, 71.87973, 32.35219, 36.1934, 41.28257, 2.7530541, -1.9872639, 22.898275, -9.932304, -15.586997, -8.986916, -20.780283, -1.1563901, 4.5018673, -9.66007, -31.509727, 1.0515851, -8.567638, -3.725089, 8.328537]</td>\n",
       "      <td>[0.5575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4425]</td>\n",
       "      <td>{'#b': 0.4425, 'b-n': 0.5575}</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-466.86948, 131.32242, -27.895485, -17.326698, -14.464546, 11.196297, -12.233515, -23.445066, -16.804337, 18.757277, 0.8819246, 1.260014, -5.4004154, 12.914358, -27.524231, 5.572198, -12.900546, -7.520482, -8.263817, 2.8675315]</td>\n",
       "      <td>[0.81916667, 0.18083333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>{'b-n': 0.8191666666666697, 'm-n': 0.1808333333333303}</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-487.07132, 135.9779, -3.025653, -9.435239, -3.4786983, 1.6876291, -20.943718, -7.8202906, 14.812183, 8.936998, 15.624887, -38.165127, -3.2568831, 6.699148, 6.8093796, -13.3633795, -4.9855833, 6.2166805, -5.063633, 0.8156262]</td>\n",
       "      <td>[0.7725, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2275]</td>\n",
       "      <td>{'#b': 0.2275, 'b-n': 0.7725}</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-504.30026, 167.75624, -43.991325, -12.616833, 17.761126, -22.669508, -22.600502, -32.012856, -13.214453, 23.409264, -22.854027, -14.643912, 1.8293163, 5.0488992, -20.995512, -6.752759, -0.42442548, -4.7568603, -8.2071056, -18.368492]</td>\n",
       "      <td>[0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4]</td>\n",
       "      <td>{'#b': 0.4, 'b-n': 0.6}</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-522.470398, 114.880325, -28.6871605, 60.610611, -33.066246, -22.5851097, -2.70186663, 11.0690565, -11.3093596, -39.6996803, -7.85373592, 0.471425563, -3.77920032, 1.12888217, -12.4821405, -17.5289898, 3.33738279, -5.42973566, -11.7449684, -10.3080702]</td>\n",
       "      <td>[0.58333333, 0.41666667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>{'b-n': 0.5833333333333303, 'm-n': 0.4166666666666697}</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32377</th>\n",
       "      <td>[-354.37424, 153.82423, 2.520186, 8.557072, -62.9833, -70.76247, 31.027435, -28.974052, -1.3362603, -6.622672, -28.985481, 6.8635836, -23.302856, 17.592358, 1.9556779, -8.322697, 1.6778979, -5.984693, -9.394501, -18.730194]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>{'#b': 1.0}</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32378</th>\n",
       "      <td>[-541.0925, -20.647453, -19.415312, -3.7550635, -25.011345, 7.143332, -16.555939, -10.724681, 13.894259, 10.9246235, -23.339613, -8.79563, 11.684788, 1.0004408, 9.840512, -4.0137005, 5.4408083, -1.9365463, -10.327684, 1.4280273]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>{'#b': 1.0}</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32379</th>\n",
       "      <td>[-361.39725, 200.15135, -11.198521, -74.57884, -26.557076, -30.454882, -26.454899, -23.900236, -6.9710026, 10.228376, -62.84133, 9.395352, -1.8819872, -2.3180122, 9.137184, -22.453075, 9.633455, -0.8796562, -26.236694, -2.5513773]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>{'#b': 1.0}</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32380</th>\n",
       "      <td>[-560.96643, 85.371414, -12.350075, -33.760818, -45.370388, 11.333093, -5.9454966, -0.63891798, 3.0575819, -33.63752, 20.533573, 1.9418074, 10.326906, 4.8501635, -0.17955418, -8.1675873, -6.8336906, 10.819086, -4.0187063, -1.4526093]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>{'#b': 1.0}</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32381</th>\n",
       "      <td>[-460.5466, 124.15584, -70.547966, -20.204004, -22.291546, -13.947313, -28.85445, -36.783035, 13.090644, -21.71759, -8.811714, -18.16044, -16.802431, 1.4008222, -27.485973, -12.374393, -11.544936, -19.798939, -9.840371, -13.991043]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>{'#b': 1.0}</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32382 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                mfcc  \\\n",
       "0                                        [-705.99225, 71.87973, 32.35219, 36.1934, 41.28257, 2.7530541, -1.9872639, 22.898275, -9.932304, -15.586997, -8.986916, -20.780283, -1.1563901, 4.5018673, -9.66007, -31.509727, 1.0515851, -8.567638, -3.725089, 8.328537]   \n",
       "1                               [-466.86948, 131.32242, -27.895485, -17.326698, -14.464546, 11.196297, -12.233515, -23.445066, -16.804337, 18.757277, 0.8819246, 1.260014, -5.4004154, 12.914358, -27.524231, 5.572198, -12.900546, -7.520482, -8.263817, 2.8675315]   \n",
       "2                                 [-487.07132, 135.9779, -3.025653, -9.435239, -3.4786983, 1.6876291, -20.943718, -7.8202906, 14.812183, 8.936998, 15.624887, -38.165127, -3.2568831, 6.699148, 6.8093796, -13.3633795, -4.9855833, 6.2166805, -5.063633, 0.8156262]   \n",
       "3                        [-504.30026, 167.75624, -43.991325, -12.616833, 17.761126, -22.669508, -22.600502, -32.012856, -13.214453, 23.409264, -22.854027, -14.643912, 1.8293163, 5.0488992, -20.995512, -6.752759, -0.42442548, -4.7568603, -8.2071056, -18.368492]   \n",
       "4      [-522.470398, 114.880325, -28.6871605, 60.610611, -33.066246, -22.5851097, -2.70186663, 11.0690565, -11.3093596, -39.6996803, -7.85373592, 0.471425563, -3.77920032, 1.12888217, -12.4821405, -17.5289898, 3.33738279, -5.42973566, -11.7449684, -10.3080702]   \n",
       "...                                                                                                                                                                                                                                                              ...   \n",
       "32377                                [-354.37424, 153.82423, 2.520186, 8.557072, -62.9833, -70.76247, 31.027435, -28.974052, -1.3362603, -6.622672, -28.985481, 6.8635836, -23.302856, 17.592358, 1.9556779, -8.322697, 1.6778979, -5.984693, -9.394501, -18.730194]   \n",
       "32378                           [-541.0925, -20.647453, -19.415312, -3.7550635, -25.011345, 7.143332, -16.555939, -10.724681, 13.894259, 10.9246235, -23.339613, -8.79563, 11.684788, 1.0004408, 9.840512, -4.0137005, 5.4408083, -1.9365463, -10.327684, 1.4280273]   \n",
       "32379                         [-361.39725, 200.15135, -11.198521, -74.57884, -26.557076, -30.454882, -26.454899, -23.900236, -6.9710026, 10.228376, -62.84133, 9.395352, -1.8819872, -2.3180122, 9.137184, -22.453075, 9.633455, -0.8796562, -26.236694, -2.5513773]   \n",
       "32380                      [-560.96643, 85.371414, -12.350075, -33.760818, -45.370388, 11.333093, -5.9454966, -0.63891798, 3.0575819, -33.63752, 20.533573, 1.9418074, 10.326906, 4.8501635, -0.17955418, -8.1675873, -6.8336906, 10.819086, -4.0187063, -1.4526093]   \n",
       "32381                        [-460.5466, 124.15584, -70.547966, -20.204004, -22.291546, -13.947313, -28.85445, -36.783035, 13.090644, -21.71759, -8.811714, -18.16044, -16.802431, 1.4008222, -27.485973, -12.374393, -11.544936, -19.798939, -9.840371, -13.991043]   \n",
       "\n",
       "                                                                                      label  \\\n",
       "0              [0.5575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4425]   \n",
       "1      [0.81916667, 0.18083333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "2              [0.7725, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2275]   \n",
       "3                    [0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4]   \n",
       "4      [0.58333333, 0.41666667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "...                                                                                     ...   \n",
       "32377                [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]   \n",
       "32378                [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]   \n",
       "32379                [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]   \n",
       "32380                [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]   \n",
       "32381                [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]   \n",
       "\n",
       "                                                state_weights  \\\n",
       "0                               {'#b': 0.4425, 'b-n': 0.5575}   \n",
       "1      {'b-n': 0.8191666666666697, 'm-n': 0.1808333333333303}   \n",
       "2                               {'#b': 0.2275, 'b-n': 0.7725}   \n",
       "3                                     {'#b': 0.4, 'b-n': 0.6}   \n",
       "4      {'b-n': 0.5833333333333303, 'm-n': 0.4166666666666697}   \n",
       "...                                                       ...   \n",
       "32377                                             {'#b': 1.0}   \n",
       "32378                                             {'#b': 1.0}   \n",
       "32379                                             {'#b': 1.0}   \n",
       "32380                                             {'#b': 1.0}   \n",
       "32381                                             {'#b': 1.0}   \n",
       "\n",
       "       single_class_label  \n",
       "0                       0  \n",
       "1                       0  \n",
       "2                       0  \n",
       "3                       0  \n",
       "4                       0  \n",
       "...                   ...  \n",
       "32377                  13  \n",
       "32378                  13  \n",
       "32379                  13  \n",
       "32380                  13  \n",
       "32381                  13  \n",
       "\n",
       "[32382 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def upsample_multiple_minority(df):\n",
    "\n",
    "    # Assuming df_train_pre is your dataframe\n",
    "    # Identify majority and minority classes\n",
    "    majority_classes = [13, 14]\n",
    "    minority_classes = list(range(13))\n",
    "\n",
    "    # Find the size of the smallest majority class\n",
    "    majority_class_size = df_train_pre['single_class_label'].value_counts().nlargest(2).iloc[1]\n",
    "    \n",
    "    # List to hold the upsampled dataframes\n",
    "    list_df = []\n",
    "\n",
    "    # Loop through each minority class and upsample\n",
    "    for class_value in minority_classes:\n",
    "        df_minority_class = df_train_pre[df_train_pre['single_class_label'] == class_value]\n",
    "        df_minority_upsampled = resample(df_minority_class, \n",
    "                                        replace=True,     # sample with replacement\n",
    "                                        n_samples=majority_class_size,    # to match majority class size\n",
    "                                        random_state=123) # reproducible results\n",
    "        list_df.append(df_minority_upsampled)\n",
    "\n",
    "    # Append majority classes without change\n",
    "    df_majority = df_train_pre[df_train_pre['single_class_label'].isin(majority_classes)]\n",
    "    list_df.append(df_majority)\n",
    "\n",
    "    # Concatenate all dataframes\n",
    "    df_upsampled = pd.concat(list_df)\n",
    "    return df_upsampled\n",
    "\n",
    "df_train = upsample_multiple_minority(df_train_pre)\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "display(df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>single_class_label</th>\n",
       "      <th>13</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8319</td>\n",
       "      <td>1851</td>\n",
       "      <td>1851</td>\n",
       "      <td>1851</td>\n",
       "      <td>1851</td>\n",
       "      <td>1851</td>\n",
       "      <td>1851</td>\n",
       "      <td>1851</td>\n",
       "      <td>1851</td>\n",
       "      <td>1851</td>\n",
       "      <td>1851</td>\n",
       "      <td>1851</td>\n",
       "      <td>1851</td>\n",
       "      <td>1851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8319</td>\n",
       "      <td>204</td>\n",
       "      <td>179</td>\n",
       "      <td>180</td>\n",
       "      <td>252</td>\n",
       "      <td>239</td>\n",
       "      <td>233</td>\n",
       "      <td>138</td>\n",
       "      <td>146</td>\n",
       "      <td>148</td>\n",
       "      <td>159</td>\n",
       "      <td>166</td>\n",
       "      <td>180</td>\n",
       "      <td>1851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "single_class_label    13    0     1     2     3     4     5     6     7   \\\n",
       "count               8319  1851  1851  1851  1851  1851  1851  1851  1851   \n",
       "count               8319   204   179   180   252   239   233   138   146   \n",
       "\n",
       "single_class_label    8     9     10    11    12  \n",
       "count               1851  1851  1851  1851  1851  \n",
       "count                148   159   166   180  1851  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "display(pd.DataFrame([df_train['single_class_label'].value_counts(),df_train_pre['single_class_label'].value_counts()]))\n",
    "\n",
    "#Questions:\n",
    "#1- MFCC's that correspond to the same phoneme are closer in the feature space.\n",
    "#   Should we take advantage of this while oversampling? (e.g. taking convex combination of two observations.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('timit/data/TRAIN/DR2/FPJF0/SX146.WAV',\n",
       "  'timit/data/TRAIN/DR2/FPJF0/SX146.PHN',\n",
       "  'timit/data/TRAIN/DR2/FPJF0/SX146.WRD'),\n",
       " ('timit/data/TRAIN/DR3/MDBB1/SX376.WAV',\n",
       "  'timit/data/TRAIN/DR3/MDBB1/SX376.PHN',\n",
       "  'timit/data/TRAIN/DR3/MDBB1/SX376.WRD'),\n",
       " ('timit/data/TRAIN/DR5/MPMB0/SX61.WAV',\n",
       "  'timit/data/TRAIN/DR5/MPMB0/SX61.PHN',\n",
       "  'timit/data/TRAIN/DR5/MPMB0/SX61.WRD'),\n",
       " ('timit/data/TRAIN/DR4/MJLS0/SX376.WAV',\n",
       "  'timit/data/TRAIN/DR4/MJLS0/SX376.PHN',\n",
       "  'timit/data/TRAIN/DR4/MJLS0/SX376.WRD'),\n",
       " ('timit/data/TRAIN/DR3/MTJM0/SX146.WAV',\n",
       "  'timit/data/TRAIN/DR3/MTJM0/SX146.PHN',\n",
       "  'timit/data/TRAIN/DR3/MTJM0/SX146.WRD'),\n",
       " ('timit/data/TRAIN/DR7/MBBR0/SX425.WAV',\n",
       "  'timit/data/TRAIN/DR7/MBBR0/SX425.PHN',\n",
       "  'timit/data/TRAIN/DR7/MBBR0/SX425.WRD'),\n",
       " ('timit/data/TRAIN/DR5/FCDR1/SX376.WAV',\n",
       "  'timit/data/TRAIN/DR5/FCDR1/SX376.PHN',\n",
       "  'timit/data/TRAIN/DR5/FCDR1/SX376.WRD'),\n",
       " ('timit/data/TRAIN/DR4/MLJC0/SI1855.WAV',\n",
       "  'timit/data/TRAIN/DR4/MLJC0/SI1855.PHN',\n",
       "  'timit/data/TRAIN/DR4/MLJC0/SI1855.WRD'),\n",
       " ('timit/data/TRAIN/DR5/MTAT0/SX210.WAV',\n",
       "  'timit/data/TRAIN/DR5/MTAT0/SX210.PHN',\n",
       "  'timit/data/TRAIN/DR5/MTAT0/SX210.WRD'),\n",
       " ('timit/data/TRAIN/DR1/FDAW0/SX146.WAV',\n",
       "  'timit/data/TRAIN/DR1/FDAW0/SX146.PHN',\n",
       "  'timit/data/TRAIN/DR1/FDAW0/SX146.WRD'),\n",
       " ('timit/data/TEST/DR1/MDAB0/SI1039.WAV',\n",
       "  'timit/data/TEST/DR1/MDAB0/SI1039.PHN',\n",
       "  'timit/data/TEST/DR1/MDAB0/SI1039.WRD'),\n",
       " ('timit/data/TRAIN/DR7/MREM0/SX61.WAV',\n",
       "  'timit/data/TRAIN/DR7/MREM0/SX61.PHN',\n",
       "  'timit/data/TRAIN/DR7/MREM0/SX61.WRD'),\n",
       " ('timit/data/TRAIN/DR2/FHLM0/SX210.WAV',\n",
       "  'timit/data/TRAIN/DR2/FHLM0/SX210.PHN',\n",
       "  'timit/data/TRAIN/DR2/FHLM0/SX210.WRD'),\n",
       " ('timit/data/TRAIN/DR4/MKAM0/SX146.WAV',\n",
       "  'timit/data/TRAIN/DR4/MKAM0/SX146.PHN',\n",
       "  'timit/data/TRAIN/DR4/MKAM0/SX146.WRD'),\n",
       " ('timit/data/TRAIN/DR4/MARW0/SX376.WAV',\n",
       "  'timit/data/TRAIN/DR4/MARW0/SX376.PHN',\n",
       "  'timit/data/TRAIN/DR4/MARW0/SX376.WRD'),\n",
       " ('timit/data/TRAIN/DR2/MRMS0/SX210.WAV',\n",
       "  'timit/data/TRAIN/DR2/MRMS0/SX210.PHN',\n",
       "  'timit/data/TRAIN/DR2/MRMS0/SX210.WRD'),\n",
       " ('timit/data/TRAIN/DR5/MCLM0/SX376.WAV',\n",
       "  'timit/data/TRAIN/DR5/MCLM0/SX376.PHN',\n",
       "  'timit/data/TRAIN/DR5/MCLM0/SX376.WRD'),\n",
       " ('timit/data/TRAIN/DR4/MDMA0/SX440.WAV',\n",
       "  'timit/data/TRAIN/DR4/MDMA0/SX440.PHN',\n",
       "  'timit/data/TRAIN/DR4/MDMA0/SX440.WRD'),\n",
       " ('timit/data/TRAIN/DR4/MDCD0/SX425.WAV',\n",
       "  'timit/data/TRAIN/DR4/MDCD0/SX425.PHN',\n",
       "  'timit/data/TRAIN/DR4/MDCD0/SX425.WRD'),\n",
       " ('timit/data/TRAIN/DR8/MRDM0/SX425.WAV',\n",
       "  'timit/data/TRAIN/DR8/MRDM0/SX425.PHN',\n",
       "  'timit/data/TRAIN/DR8/MRDM0/SX425.WRD'),\n",
       " ('timit/data/TRAIN/DR5/MJRG0/SX376.WAV',\n",
       "  'timit/data/TRAIN/DR5/MJRG0/SX376.PHN',\n",
       "  'timit/data/TRAIN/DR5/MJRG0/SX376.WRD'),\n",
       " ('timit/data/TRAIN/DR8/FBCG1/SI1612.WAV',\n",
       "  'timit/data/TRAIN/DR8/FBCG1/SI1612.PHN',\n",
       "  'timit/data/TRAIN/DR8/FBCG1/SI1612.WRD'),\n",
       " ('timit/data/TRAIN/DR3/MDTB0/SX210.WAV',\n",
       "  'timit/data/TRAIN/DR3/MDTB0/SX210.PHN',\n",
       "  'timit/data/TRAIN/DR3/MDTB0/SX210.WRD'),\n",
       " ('timit/data/TRAIN/DR1/MTPF0/SX425.WAV',\n",
       "  'timit/data/TRAIN/DR1/MTPF0/SX425.PHN',\n",
       "  'timit/data/TRAIN/DR1/MTPF0/SX425.WRD'),\n",
       " ('timit/data/TRAIN/DR7/MDLC1/SI2065.WAV',\n",
       "  'timit/data/TRAIN/DR7/MDLC1/SI2065.PHN',\n",
       "  'timit/data/TRAIN/DR7/MDLC1/SI2065.WRD'),\n",
       " ('timit/data/TRAIN/DR7/FKDE0/SX61.WAV',\n",
       "  'timit/data/TRAIN/DR7/FKDE0/SX61.PHN',\n",
       "  'timit/data/TRAIN/DR7/FKDE0/SX61.WRD'),\n",
       " ('timit/data/TRAIN/DR3/MDWM0/SX376.WAV',\n",
       "  'timit/data/TRAIN/DR3/MDWM0/SX376.PHN',\n",
       "  'timit/data/TRAIN/DR3/MDWM0/SX376.WRD'),\n",
       " ('timit/data/TRAIN/DR2/FMMH0/SI907.WAV',\n",
       "  'timit/data/TRAIN/DR2/FMMH0/SI907.PHN',\n",
       "  'timit/data/TRAIN/DR2/FMMH0/SI907.WRD'),\n",
       " ('timit/data/TRAIN/DR3/FJLR0/SX61.WAV',\n",
       "  'timit/data/TRAIN/DR3/FJLR0/SX61.PHN',\n",
       "  'timit/data/TRAIN/DR3/FJLR0/SX61.WRD')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load('processed_data/train_test_dataset_never.joblib')['test'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset into a format that torch can read.\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None, train=True):\n",
    "        # Convert the DataFrame to tensors or appropriate formats initially\n",
    "        self.mfcc = torch.tensor(np.vstack(dataframe['mfcc'].to_list()), dtype=torch.float32)\n",
    "        self.label = torch.tensor(np.vstack(dataframe['label'].to_list()), dtype=torch.long)\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mfcc)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mfcc = self.mfcc[idx]\n",
    "        label = self.label[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            mfcc = self.transform(mfcc)\n",
    "\n",
    "        return mfcc, label\n",
    "\n",
    "# Create an instance of your dataset with your DataFrame\n",
    "dataset_train = CustomDataset(df_train, train=True)  # Assuming df is your pandas DataFrame\n",
    "dataset_val = CustomDataset(df_val, train=True)\n",
    "dataset_test = CustomDataset(df_test,train=False)\n",
    "\n",
    "\n",
    "# Create the DataLoader to handle batching\n",
    "loader_train = DataLoader(dataset_train, batch_size=minibatch_size,\n",
    "                          sampler=sampler.SubsetRandomSampler(range(len(df_train))))\n",
    "\n",
    "loader_val = DataLoader(dataset_val, batch_size=1,\n",
    "                        sampler=sampler.SequentialSampler(range(len(df_val))))\n",
    "\n",
    "loader_test = DataLoader(dataset_test, batch_size=1,\n",
    "                        sampler=sampler.SequentialSampler(range(len(df_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1\n",
      "MFCC size: torch.Size([64, 20])\n",
      "Label size: torch.Size([64, 14])\n",
      "Batch: 2\n",
      "MFCC size: torch.Size([64, 20])\n",
      "Label size: torch.Size([64, 14])\n",
      "Batch: 3\n",
      "MFCC size: torch.Size([64, 20])\n",
      "Label size: torch.Size([64, 14])\n",
      "Batch: 4\n",
      "MFCC size: torch.Size([64, 20])\n",
      "Label size: torch.Size([64, 14])\n"
     ]
    }
   ],
   "source": [
    "for i, (x, y) in enumerate(loader_train):\n",
    "    print(f'Batch: {i+1}')\n",
    "    print(f'MFCC size: {x.shape}')\n",
    "    print(f'Label size: {y.shape}')\n",
    "    if i >2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 14])\n"
     ]
    }
   ],
   "source": [
    "class DNN_FC(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "        # assign layer objects to class attributes\n",
    "        # We may write a loop if we use the same activation function for all layers.\n",
    "        self.fc1 = nn.Linear(input_size, input_size)\n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        self.fc2 = nn.Linear(input_size, input_size)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight) \n",
    "        self.fc3 = nn.Linear(input_size, input_size)\n",
    "        nn.init.kaiming_normal_(self.fc3.weight)\n",
    "        self.fc4 = nn.Linear(input_size, input_size)\n",
    "        nn.init.kaiming_normal_(self.fc4.weight)\n",
    "        self.fc5 = nn.Linear(input_size, num_classes)\n",
    "        nn.init.kaiming_normal_(self.fc5.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_temp = flatten(x) # Flatten the batch to convert dimensions from (N,C,1) to (N,-1)\n",
    "        x_temp = F.relu(self.fc1(x_temp))\n",
    "        x_temp = F.relu(self.fc2(x_temp))\n",
    "        x_temp = F.relu(self.fc3(x_temp))\n",
    "        x_temp = F.relu(self.fc4(x_temp))\n",
    "        scores = self.fc5(x_temp)\n",
    "        return scores\n",
    "\n",
    "\n",
    "def test_DNN_FC():\n",
    "    input_size = len(df_train.iloc[0]['mfcc'])  # Feature dimension for mfcc\n",
    "    num_classes = len(df_train.iloc[0]['label']) # Number of phoneme classes \n",
    "    x = torch.zeros((minibatch_size, input_size), dtype=dtype)  # minibatch size 64, feature dimension 20\n",
    "    model = DNN_FC(input_size, num_classes)\n",
    "    scores = model(x)\n",
    "    print(scores.size())  # you should see [minibatch_size, num_classes]\n",
    "test_DNN_FC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, scheduler, epochs=1, print_every=50):\n",
    "\n",
    "    \"\"\"\n",
    "    Train the model using the PyTorch Module API.\n",
    "\n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    - scheduler: Learning rate scheduler \n",
    "\n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    val_loss_lst = []\n",
    "    train_loss_lst = []\n",
    "    accuracy_val_max = 0\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            y = flatten(y) # Flatten y to convert the dimension from (Nx1) to (N,)\n",
    "            x = x.to(device=device)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device).long() \n",
    "            scores = model(x)\n",
    "\n",
    "            # Compare the output vector with the label vector using BCEwithLogitsLoss.\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            loss = criterion(scores, y)  # nn.KLDivLoss expects the NN output to be in the log-softmax scale. \n",
    "            \n",
    "            # criterion = nn.BCEWithLogitsLoss() \n",
    "            # loss = criterion(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "            \n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "            \n",
    "            if t % print_every == 0:\n",
    "                print('Epoch %d, Iteration %d, loss = %.4f' % (e, t, loss.item()))\n",
    "                print()\n",
    "                accuracy_val = check_accuracy(loader_val, model)\n",
    "                val_loss = check_loss(loader_val, model) \n",
    "                if accuracy_val > accuracy_val_max:\n",
    "                    accuracy_val_max = accuracy_val\n",
    "                    model_params = model.state_dict()\n",
    "                train_loss_lst.append(loss.item())\n",
    "                val_loss_lst.append(val_loss.item()) \n",
    "        \n",
    "        # Update the learning rate at every epoch.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Plot the accuracy values\n",
    "    plt.plot(val_loss_lst, label='Validation Loss')\n",
    "    plt.plot(train_loss_lst, label='Training Loss')\n",
    "\n",
    "    # Add labels and title to the plot\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    return train_loss_lst, val_loss_lst\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected floating point type for target with class probabilities, got Long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate) \n\u001b[1;32m      6\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mExponentialLR(optimizer, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m train_loss_lst, val_loss_lst \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, scheduler, epochs, print_every)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Compare the output vector with the label vector using BCEwithLogitsLoss.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m---> 28\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# nn.KLDivLoss expects the NN output to be in the log-softmax scale. \u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# criterion = nn.BCEWithLogitsLoss() \u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# loss = criterion(scores, y)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Zero out all of the gradients for the variables which the optimizer will update.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/loss.py:1185\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/functional.py:3086\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3085\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3086\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected floating point type for target with class probabilities, got Long"
     ]
    }
   ],
   "source": [
    "'''\n",
    "input_size = len(df_train.iloc[0]['mfcc'])\n",
    "num_classes = len(df_train.iloc[0]['label'])\n",
    "learning_rate = 1e-2 \n",
    "model = DNN_FC(input_size, num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate) \n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "train_loss_lst, val_loss_lst = train(model, optimizer, scheduler, epochs = 10)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_loss_lst)\n",
    "print(val_loss_lst) \n",
    "\n",
    "# Plot the accuracy values\n",
    "plt.plot(val_loss_lst[2:], label='Validation Loss')\n",
    "plt.plot(train_loss_lst[2:], label='Training Loss')\n",
    "\n",
    "# Add labels and title to the plot\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNN_utils.check_accuracy(loader_val, model)\n",
    "DNN_utils.check_accuracy(loader_test, model) \n",
    "DNN_utils.check_accuracy(loader_train, model) \n",
    "\n",
    "loader_train_fortest = DataLoader(dataset_train, batch_size=1,\n",
    "                                  sampler = sampler.SequentialSampler(range(len(df_train))))\n",
    "DNN_utils.check_accuracy(loader_train_fortest, model) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model = DNN_FC(input_size, num_classes) \n",
    "input_size = len(df_train.iloc[0]['mfcc'])\n",
    "num_classes = len(df_train.iloc[0]['label'])\n",
    "\n",
    "# Specify the file path\n",
    "file_path = \"model_parameters.pth\"\n",
    "\n",
    "# Load the model parameters\n",
    "saved_model.load_state_dict(torch.load(file_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on validation set\n",
      "Got 2188 / 3099 correct (70.60)\n",
      "Checking accuracy on test set\n",
      "Got 12121 / 17249 correct (70.27)\n",
      "Checking accuracy on validation set\n",
      "Got 8775 / 32382 correct (27.10)\n",
      "Checking accuracy on validation set\n",
      "Got 8775 / 32382 correct (27.10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.27098387993329626"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DNN_utils.check_accuracy(loader_val, saved_model)\n",
    "DNN_utils.check_accuracy(loader_test, saved_model) \n",
    "DNN_utils.check_accuracy(loader_train, saved_model) \n",
    "\n",
    "loader_train_fortest = DataLoader(dataset_train, batch_size=1,\n",
    "                                  sampler = sampler.SequentialSampler(range(len(df_train))))\n",
    "DNN_utils.check_accuracy(loader_train_fortest, saved_model) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_probabilities(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print('Getting estimated probabilities on validation set')\n",
    "    else:\n",
    "        print('Getting estimated probabilities on test set') \n",
    "    model.eval()  # set model to evaluation mode\n",
    "    probabilities_dict = {} \n",
    "    batch_size = loader.batch_size\n",
    "    with torch.no_grad():\n",
    "        for idx, (x, y) in enumerate(loader):\n",
    "            y = flatten(y) # Flatten y to convert dimension from (Nx1) to (N,)\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=dtype) \n",
    "            scores = model(x) \n",
    "            probabilities = torch.softmax(scores, dim=1) \n",
    "            \n",
    "            # Save the probabilities with the corresponding row index\n",
    "            for i in range(len(probabilities)):\n",
    "                probabilities_dict[idx * batch_size + i] = probabilities[i].numpy()\n",
    "    \n",
    "    return probabilities_dict\n",
    "\n",
    "\n",
    "def find_emission(loader, model, scale = False):\n",
    "    '''\n",
    "    Find emission probabilities for a given data loader and model.\n",
    "    Consider changing this function if it takes too long. Currently: O(n) \n",
    "    Args:\n",
    "        loader: torch Data loader\n",
    "        model: torch DNN model\n",
    "        scale: Boolean: Set true to scale the output probability of DNN.\n",
    "    Returns:\n",
    "        emission_df: Dataframe for emission probabilities.\n",
    "    '''\n",
    "    # Get the inferred probabilities for each class (12 states, background and silence)\n",
    "    probabilities_dict = infer_probabilities(loader, model) \n",
    "    emission = probabilities_dict\n",
    "    # Get the prior vector and the transition probabilities. We don't need the transition probabilities.\n",
    "    prior_vector, _ = get_prob.main(rerun=False) \n",
    "\n",
    "    # For each key=row_idx and val=prob_array, convert the inferred probabilities into emission.\n",
    "    for key, val in emission.items():\n",
    "        # Slice val to exclude the probabilities for background and silence.\n",
    "        if scale == True:\n",
    "            log_prob = np.where(val > 0, np.log(val), -np.inf)   # Get the log probabilities. \n",
    "            log_prob = log_prob[:-2]  # Exclude the background and silence in the emission probability calculation. \n",
    "            emission[key] = [log_prob-prior_vector]  # Divide by prior vector in the log space. \n",
    "        else:\n",
    "            emission[key] = [val] \n",
    "\n",
    "    emission_df = pd.DataFrame.from_dict(emission, orient='index', columns=['Emission']) \n",
    "    return emission_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For start 45760 and end 46160, there is no time-overlapping row.\n",
      "For start 65120 and end 65520, there is no time-overlapping row.\n",
      "For start 35440 and end 35840, there is no time-overlapping row.\n",
      "For start 42480 and end 42880, there is no time-overlapping row.\n",
      "For start 49040 and end 49440, there is no time-overlapping row.\n",
      "For start 59200 and end 59600, there is no time-overlapping row.\n",
      "For start 61040 and end 61440, there is no time-overlapping row.\n",
      "For start 39840 and end 40240, there is no time-overlapping row.\n",
      "For start 46800 and end 47200, there is no time-overlapping row.\n",
      "For start 60640 and end 61040, there is no time-overlapping row.\n",
      "For start 56240 and end 56640, there is no time-overlapping row.\n",
      "For start 52240 and end 52640, there is no time-overlapping row.\n",
      "Getting estimated probabilities on test set\n",
      "Getting estimated probabilities on test set\n"
     ]
    }
   ],
   "source": [
    "prior_vector, _ = get_prob.main(rerun=True)\n",
    "estimate_prob = infer_probabilities(loader_test, model)\n",
    "emission_data = find_emission(loader_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(emission_data.iloc[0]['Emission'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting estimated probabilities on test set\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emission</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[5.173344e-17, 6.120392e-23, 2.0605932e-11, 9.465932e-12, 5.3322256e-06, 3.0784056e-05, 1.0712865e-20, 7.5062485e-07, 4.8062308e-35, 2.3023976e-13, 1.8673664e-07, 1.423909e-28, 0.53661686, 0.46334612]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[2.7787693e-17, 9.537536e-24, 1.0677138e-11, 3.5048791e-12, 4.688974e-06, 2.5838719e-05, 5.8799286e-21, 1.5398612e-06, 2.1962397e-35, 1.4828275e-13, 2.027408e-07, 9.757805e-29, 0.71491474, 0.28505307]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[6.998184e-17, 7.781263e-24, 1.3595965e-11, 4.856749e-12, 6.3809853e-06, 1.2101964e-05, 9.752834e-21, 5.6644167e-06, 5.01677e-35, 5.2787706e-13, 9.4420267e-07, 2.2619855e-28, 0.7674287, 0.23254623]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[2.5042545e-16, 1.7854215e-23, 6.291111e-12, 6.859706e-12, 1.0774825e-05, 4.0137293e-06, 5.342014e-21, 2.918402e-06, 1.5437594e-35, 2.6352462e-12, 4.096724e-06, 1.7958405e-28, 0.6176022, 0.3823759]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[2.6210614e-16, 1.2530076e-22, 1.0357863e-11, 1.4791376e-11, 8.680606e-06, 7.376133e-06, 8.400631e-21, 1.5381328e-06, 3.153503e-35, 3.3971179e-12, 2.0572247e-06, 5.00163e-28, 0.3908442, 0.6091361]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>[8.6517884e-17, 1.4219562e-24, 3.6439518e-11, 2.4013114e-12, 6.3889743e-06, 1.0419658e-05, 1.3711779e-20, 7.3334313e-06, 1.3583143e-34, 8.7380386e-14, 1.4049484e-06, 2.614563e-29, 0.9301868, 0.06978762]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>[6.585328e-17, 1.7821863e-25, 6.731849e-12, 6.118342e-13, 7.818898e-06, 3.3083938e-06, 1.4325543e-21, 2.211141e-06, 2.8357356e-36, 6.9645834e-14, 1.5368756e-06, 1.2047304e-30, 0.92814225, 0.07184287]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>[4.821921e-17, 5.0479286e-27, 1.4460076e-12, 8.936295e-14, 8.59002e-06, 5.934953e-07, 3.1998049e-22, 1.2487617e-06, 2.875243e-37, 2.7855913e-14, 1.1575687e-06, 5.4721707e-32, 0.9761346, 0.02385378]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>[1.2331175e-17, 4.208773e-27, 1.1492836e-11, 1.5196304e-13, 3.5171854e-06, 1.8428162e-06, 2.690167e-21, 2.991768e-06, 1.2464048e-35, 2.565094e-15, 1.5951083e-07, 1.0977122e-31, 0.991508, 0.008483543]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>[7.503677e-18, 6.371372e-26, 1.2380999e-11, 5.03805e-13, 3.1350112e-06, 7.807253e-06, 1.1529565e-21, 1.8045707e-06, 1.7751453e-36, 6.9349078e-15, 1.9459016e-07, 2.8873933e-31, 0.9415966, 0.058390453]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>407 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                       Emission\n",
       "0      [5.173344e-17, 6.120392e-23, 2.0605932e-11, 9.465932e-12, 5.3322256e-06, 3.0784056e-05, 1.0712865e-20, 7.5062485e-07, 4.8062308e-35, 2.3023976e-13, 1.8673664e-07, 1.423909e-28, 0.53661686, 0.46334612]\n",
       "1      [2.7787693e-17, 9.537536e-24, 1.0677138e-11, 3.5048791e-12, 4.688974e-06, 2.5838719e-05, 5.8799286e-21, 1.5398612e-06, 2.1962397e-35, 1.4828275e-13, 2.027408e-07, 9.757805e-29, 0.71491474, 0.28505307]\n",
       "2         [6.998184e-17, 7.781263e-24, 1.3595965e-11, 4.856749e-12, 6.3809853e-06, 1.2101964e-05, 9.752834e-21, 5.6644167e-06, 5.01677e-35, 5.2787706e-13, 9.4420267e-07, 2.2619855e-28, 0.7674287, 0.23254623]\n",
       "3         [2.5042545e-16, 1.7854215e-23, 6.291111e-12, 6.859706e-12, 1.0774825e-05, 4.0137293e-06, 5.342014e-21, 2.918402e-06, 1.5437594e-35, 2.6352462e-12, 4.096724e-06, 1.7958405e-28, 0.6176022, 0.3823759]\n",
       "4          [2.6210614e-16, 1.2530076e-22, 1.0357863e-11, 1.4791376e-11, 8.680606e-06, 7.376133e-06, 8.400631e-21, 1.5381328e-06, 3.153503e-35, 3.3971179e-12, 2.0572247e-06, 5.00163e-28, 0.3908442, 0.6091361]\n",
       "..                                                                                                                                                                                                          ...\n",
       "402  [8.6517884e-17, 1.4219562e-24, 3.6439518e-11, 2.4013114e-12, 6.3889743e-06, 1.0419658e-05, 1.3711779e-20, 7.3334313e-06, 1.3583143e-34, 8.7380386e-14, 1.4049484e-06, 2.614563e-29, 0.9301868, 0.06978762]\n",
       "403     [6.585328e-17, 1.7821863e-25, 6.731849e-12, 6.118342e-13, 7.818898e-06, 3.3083938e-06, 1.4325543e-21, 2.211141e-06, 2.8357356e-36, 6.9645834e-14, 1.5368756e-06, 1.2047304e-30, 0.92814225, 0.07184287]\n",
       "404       [4.821921e-17, 5.0479286e-27, 1.4460076e-12, 8.936295e-14, 8.59002e-06, 5.934953e-07, 3.1998049e-22, 1.2487617e-06, 2.875243e-37, 2.7855913e-14, 1.1575687e-06, 5.4721707e-32, 0.9761346, 0.02385378]\n",
       "405     [1.2331175e-17, 4.208773e-27, 1.1492836e-11, 1.5196304e-13, 3.5171854e-06, 1.8428162e-06, 2.690167e-21, 2.991768e-06, 1.2464048e-35, 2.565094e-15, 1.5951083e-07, 1.0977122e-31, 0.991508, 0.008483543]\n",
       "406     [7.503677e-18, 6.371372e-26, 1.2380999e-11, 5.03805e-13, 3.1350112e-06, 7.807253e-06, 1.1529565e-21, 1.8045707e-06, 1.7751453e-36, 6.9349078e-15, 1.9459016e-07, 2.8873933e-31, 0.9415966, 0.058390453]\n",
       "\n",
       "[407 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def path_to_emission(model, file_path_wav: str, file_path_phn: str):\n",
    "    '''\n",
    "    Given the path of a file, get the emission probabilities.\n",
    "    Args:\n",
    "        model: DNN Model\n",
    "        file_path_vaw: Path of the audio file as a string.\n",
    "        file_path_phn: file path for the phonemes.\n",
    "    Returns:\n",
    "        emit: pd.dataframe\n",
    "            Emission probabilities for each frame in the audio file.\n",
    "    '''\n",
    "    df_test = mfcc_label.prepare_data(file_path_phn,file_path_wav)\n",
    "    DNN_utils.column_str_to_numpy(df_test, 'mfcc')\n",
    "    DNN_utils.column_str_to_numpy(df_test, 'label')\n",
    "    # Convert dataframe into a loader so that torch can work with.\n",
    "    dataset_test = CustomDataset(df_test,train=False)\n",
    "    loader_test = DataLoader(dataset_test, batch_size=1,\n",
    "                        sampler=sampler.SequentialSampler(range(len(df_test))))\n",
    "\n",
    "    emission_data = find_emission(loader_test, model)\n",
    "    return emission_data\n",
    "    \n",
    "\n",
    "path_to_emission(saved_model,'timit/data/TRAIN/DR4/MDCD0/SX425.WAV','timit/data/TRAIN/DR4/MDCD0/SX425.PHN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting estimated probabilities on test set\n",
      "For start 45760 and end 46160, there is no time-overlapping row.\n",
      "Caution: There is no time-overlapping rows for start 45760 and end 46160\n",
      "Getting estimated probabilities on test set\n",
      "Getting estimated probabilities on test set\n",
      "For start 65120 and end 65520, there is no time-overlapping row.\n",
      "Caution: There is no time-overlapping rows for start 65120 and end 65520\n",
      "Getting estimated probabilities on test set\n",
      "Getting estimated probabilities on test set\n",
      "For start 35440 and end 35840, there is no time-overlapping row.\n",
      "Caution: There is no time-overlapping rows for start 35440 and end 35840\n",
      "Getting estimated probabilities on test set\n",
      "Getting estimated probabilities on test set\n",
      "Getting estimated probabilities on test set\n",
      "Getting estimated probabilities on test set\n",
      "For start 42480 and end 42880, there is no time-overlapping row.\n",
      "Caution: There is no time-overlapping rows for start 42480 and end 42880\n",
      "Getting estimated probabilities on test set\n",
      "Getting estimated probabilities on test set\n",
      "Getting estimated probabilities on test set\n",
      "Getting estimated probabilities on test set\n",
      "Getting estimated probabilities on test set\n",
      "For start 49040 and end 49440, there is no time-overlapping row.\n",
      "Caution: There is no time-overlapping rows for start 49040 and end 49440\n",
      "Getting estimated probabilities on test set\n",
      "For start 59200 and end 59600, there is no time-overlapping row.\n",
      "Caution: There is no time-overlapping rows for start 59200 and end 59600\n",
      "Getting estimated probabilities on test set\n",
      "Getting estimated probabilities on test set\n",
      "Getting estimated probabilities on test set\n",
      "Getting estimated probabilities on test set\n",
      "Getting estimated probabilities on test set\n",
      "For start 61040 and end 61440, there is no time-overlapping row.\n",
      "Caution: There is no time-overlapping rows for start 61040 and end 61440\n",
      "Getting estimated probabilities on test set\n",
      "For start 39840 and end 40240, there is no time-overlapping row.\n",
      "Caution: There is no time-overlapping rows for start 39840 and end 40240\n",
      "Getting estimated probabilities on test set\n",
      "For start 46800 and end 47200, there is no time-overlapping row.\n",
      "Caution: There is no time-overlapping rows for start 46800 and end 47200\n",
      "Getting estimated probabilities on test set\n",
      "For start 60640 and end 61040, there is no time-overlapping row.\n",
      "Caution: There is no time-overlapping rows for start 60640 and end 61040\n",
      "Getting estimated probabilities on test set\n",
      "For start 56240 and end 56640, there is no time-overlapping row.\n",
      "Caution: There is no time-overlapping rows for start 56240 and end 56640\n",
      "Getting estimated probabilities on test set\n",
      "Getting estimated probabilities on test set\n",
      "Getting estimated probabilities on test set\n",
      "For start 52240 and end 52640, there is no time-overlapping row.\n",
      "Caution: There is no time-overlapping rows for start 52240 and end 52640\n",
      "Getting estimated probabilities on test set\n"
     ]
    }
   ],
   "source": [
    "def get_emission_all_paths(model, path_type: str = 'test'):\n",
    "    from joblib import dump\n",
    "    paths = load('processed_data/train_test_dataset_never.joblib')[path_type]\n",
    "    data = {}\n",
    "    for i in range(len(paths)):\n",
    "        file_path_wav, file_path_phn, file_path_word = paths[i]\n",
    "        emission_data = path_to_emission(model, file_path_wav, file_path_phn)\n",
    "        data[(file_path_wav, file_path_phn, file_path_word)] = emission_data\n",
    "        dump(data, \"processed_data/\"+path_type+\"_data_for_hmm.joblib\") \n",
    "    return data\n",
    "    \n",
    "data = get_emission_all_paths(saved_model, path_type='train')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data[('timit/data/TRAIN/DR2/FPJF0/SX146.WAV',\n",
    "  'timit/data/TRAIN/DR2/FPJF0/SX146.PHN',\n",
    "  'timit/data/TRAIN/DR2/FPJF0/SX146.WRD')]\n",
    ") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F  \n",
    "from DNN_utils import (flatten) \n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from joblib import load \n",
    "\n",
    "# Import helper functions.\n",
    "import mfcc_label \n",
    "import get_prob\n",
    "\n",
    "\n",
    "df_train = pd.read_csv('processed_data/dnn_never_train.csv')\n",
    "df_test = pd.read_csv('processed_data/dnn_never_test.csv')\n",
    "\n",
    "def column_str_to_numpy(df, colname:str):\n",
    "    # Given pd.DataFrame df, convert the column colname from string to numpy array.\n",
    "    if isinstance(df.iloc[0][colname], str):\n",
    "        df[colname]=df[colname].apply(lambda x: np.fromstring(x[1:-1], sep=' '))\n",
    "\n",
    "column_str_to_numpy(df_train, 'mfcc')\n",
    "column_str_to_numpy(df_train, 'label')\n",
    "column_str_to_numpy(df_test, 'mfcc')\n",
    "column_str_to_numpy(df_test, 'label')\n",
    "\n",
    "# Configurations \n",
    "NUM_TRAIN = int(0.8*len(df_train)) # Number of training examples for splitting training and validation datasets. \n",
    "NUM_ROWS = len(df_train)\n",
    "device = 'cpu'\n",
    "dtype = torch.float32\n",
    "print_every = 50\n",
    "\n",
    "# DNN Architecture Hyperparameters\n",
    "minibatch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mfcc</th>\n",
       "      <th>label</th>\n",
       "      <th>state_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-861.406067, 25.538353, 28.6545143, 1.8917164...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>{'h#': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-824.945, 20.405128, 15.508747, -2.5022912, 2...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>{'h#': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-818.1322, 16.536594, 10.08112, -9.460713, 13...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>{'h#': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-816.3057, 14.217917, 15.505737, -9.549528, 1...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>{'h#': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-809.08112, 17.429758, 14.575484, -10.453241,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>{'h#': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17244</th>\n",
       "      <td>[-719.52673, 54.290421, -17.710644, -19.354218...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>{'h#': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17245</th>\n",
       "      <td>[-717.0269, 61.623383, -12.215822, -7.030402, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>{'h#': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17246</th>\n",
       "      <td>[-713.20428, 57.323204, -11.463234, -5.8994236...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>{'h#': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17247</th>\n",
       "      <td>[-718.0743, 46.839127, -18.039227, -11.385343,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>{'h#': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17248</th>\n",
       "      <td>[-722.76978, 41.588303, -25.800964, -14.115209...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>{'h#': 1.0}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17249 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    mfcc  \\\n",
       "0      [-861.406067, 25.538353, 28.6545143, 1.8917164...   \n",
       "1      [-824.945, 20.405128, 15.508747, -2.5022912, 2...   \n",
       "2      [-818.1322, 16.536594, 10.08112, -9.460713, 13...   \n",
       "3      [-816.3057, 14.217917, 15.505737, -9.549528, 1...   \n",
       "4      [-809.08112, 17.429758, 14.575484, -10.453241,...   \n",
       "...                                                  ...   \n",
       "17244  [-719.52673, 54.290421, -17.710644, -19.354218...   \n",
       "17245  [-717.0269, 61.623383, -12.215822, -7.030402, ...   \n",
       "17246  [-713.20428, 57.323204, -11.463234, -5.8994236...   \n",
       "17247  [-718.0743, 46.839127, -18.039227, -11.385343,...   \n",
       "17248  [-722.76978, 41.588303, -25.800964, -14.115209...   \n",
       "\n",
       "                                                   label state_weights  \n",
       "0      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   {'h#': 1.0}  \n",
       "1      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   {'h#': 1.0}  \n",
       "2      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   {'h#': 1.0}  \n",
       "3      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   {'h#': 1.0}  \n",
       "4      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   {'h#': 1.0}  \n",
       "...                                                  ...           ...  \n",
       "17244  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   {'h#': 1.0}  \n",
       "17245  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   {'h#': 1.0}  \n",
       "17246  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   {'h#': 1.0}  \n",
       "17247  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   {'h#': 1.0}  \n",
       "17248  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   {'h#': 1.0}  \n",
       "\n",
       "[17249 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "isinstance(df_test.iloc[0]['mfcc'],str)\n",
    "print(type(df_test.iloc[0]['label']))\n",
    "# If the type is str, convert it\n",
    "\n",
    "display(df_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('timit/data/TRAIN/DR4/MDCD0/SX425.WAV',\n",
       "  'timit/data/TRAIN/DR4/MDCD0/SX425.PHN',\n",
       "  'timit/data/TRAIN/DR4/MDCD0/SX425.WRD'),\n",
       " ('timit/data/TRAIN/DR4/MJLS0/SX376.WAV',\n",
       "  'timit/data/TRAIN/DR4/MJLS0/SX376.PHN',\n",
       "  'timit/data/TRAIN/DR4/MJLS0/SX376.WRD'),\n",
       " ('timit/data/TRAIN/DR1/FDAW0/SX146.WAV',\n",
       "  'timit/data/TRAIN/DR1/FDAW0/SX146.PHN',\n",
       "  'timit/data/TRAIN/DR1/FDAW0/SX146.WRD'),\n",
       " ('timit/data/TRAIN/DR7/MREM0/SX61.WAV',\n",
       "  'timit/data/TRAIN/DR7/MREM0/SX61.PHN',\n",
       "  'timit/data/TRAIN/DR7/MREM0/SX61.WRD'),\n",
       " ('timit/data/TRAIN/DR6/MBMA1/SI2214.WAV',\n",
       "  'timit/data/TRAIN/DR6/MBMA1/SI2214.PHN',\n",
       "  'timit/data/TRAIN/DR6/MBMA1/SI2214.WRD'),\n",
       " ('timit/data/TRAIN/DR4/MDMA0/SX440.WAV',\n",
       "  'timit/data/TRAIN/DR4/MDMA0/SX440.PHN',\n",
       "  'timit/data/TRAIN/DR4/MDMA0/SX440.WRD'),\n",
       " ('timit/data/TRAIN/DR4/FALR0/SX425.WAV',\n",
       "  'timit/data/TRAIN/DR4/FALR0/SX425.PHN',\n",
       "  'timit/data/TRAIN/DR4/FALR0/SX425.WRD'),\n",
       " ('timit/data/TEST/DR1/MDAB0/SI1039.WAV',\n",
       "  'timit/data/TEST/DR1/MDAB0/SI1039.PHN',\n",
       "  'timit/data/TEST/DR1/MDAB0/SI1039.WRD'),\n",
       " ('timit/data/TRAIN/DR5/MJDM0/SX440.WAV',\n",
       "  'timit/data/TRAIN/DR5/MJDM0/SX440.PHN',\n",
       "  'timit/data/TRAIN/DR5/MJDM0/SX440.WRD'),\n",
       " ('timit/data/TRAIN/DR3/MDWM0/SX376.WAV',\n",
       "  'timit/data/TRAIN/DR3/MDWM0/SX376.PHN',\n",
       "  'timit/data/TRAIN/DR3/MDWM0/SX376.WRD'),\n",
       " ('timit/data/TEST/DR5/MBPM0/SI1584.WAV',\n",
       "  'timit/data/TEST/DR5/MBPM0/SI1584.PHN',\n",
       "  'timit/data/TEST/DR5/MBPM0/SI1584.WRD'),\n",
       " ('timit/data/TRAIN/DR2/MKDT0/SI893.WAV',\n",
       "  'timit/data/TRAIN/DR2/MKDT0/SI893.PHN',\n",
       "  'timit/data/TRAIN/DR2/MKDT0/SI893.WRD'),\n",
       " ('timit/data/TRAIN/DR7/MDLC1/SI2065.WAV',\n",
       "  'timit/data/TRAIN/DR7/MDLC1/SI2065.PHN',\n",
       "  'timit/data/TRAIN/DR7/MDLC1/SI2065.WRD'),\n",
       " ('timit/data/TRAIN/DR3/MDHS0/SI2160.WAV',\n",
       "  'timit/data/TRAIN/DR3/MDHS0/SI2160.PHN',\n",
       "  'timit/data/TRAIN/DR3/MDHS0/SI2160.WRD'),\n",
       " ('timit/data/TRAIN/DR4/MGAG0/SX61.WAV',\n",
       "  'timit/data/TRAIN/DR4/MGAG0/SX61.PHN',\n",
       "  'timit/data/TRAIN/DR4/MGAG0/SX61.WRD'),\n",
       " ('timit/data/TRAIN/DR3/MRJB1/SX210.WAV',\n",
       "  'timit/data/TRAIN/DR3/MRJB1/SX210.PHN',\n",
       "  'timit/data/TRAIN/DR3/MRJB1/SX210.WRD'),\n",
       " ('timit/data/TRAIN/DR1/MJEB1/SI2097.WAV',\n",
       "  'timit/data/TRAIN/DR1/MJEB1/SI2097.PHN',\n",
       "  'timit/data/TRAIN/DR1/MJEB1/SI2097.WRD'),\n",
       " ('timit/data/TRAIN/DR3/FJLR0/SX61.WAV',\n",
       "  'timit/data/TRAIN/DR3/FJLR0/SX61.PHN',\n",
       "  'timit/data/TRAIN/DR3/FJLR0/SX61.WRD'),\n",
       " ('timit/data/TRAIN/DR4/MJMM0/SI625.WAV',\n",
       "  'timit/data/TRAIN/DR4/MJMM0/SI625.PHN',\n",
       "  'timit/data/TRAIN/DR4/MJMM0/SI625.WRD'),\n",
       " ('timit/data/TRAIN/DR2/MRMS0/SX210.WAV',\n",
       "  'timit/data/TRAIN/DR2/MRMS0/SX210.PHN',\n",
       "  'timit/data/TRAIN/DR2/MRMS0/SX210.WRD'),\n",
       " ('timit/data/TRAIN/DR8/FMBG0/SX440.WAV',\n",
       "  'timit/data/TRAIN/DR8/FMBG0/SX440.PHN',\n",
       "  'timit/data/TRAIN/DR8/FMBG0/SX440.WRD'),\n",
       " ('timit/data/TRAIN/DR4/FLKM0/SX440.WAV',\n",
       "  'timit/data/TRAIN/DR4/FLKM0/SX440.PHN',\n",
       "  'timit/data/TRAIN/DR4/FLKM0/SX440.WRD'),\n",
       " ('timit/data/TRAIN/DR4/MCSS0/SX210.WAV',\n",
       "  'timit/data/TRAIN/DR4/MCSS0/SX210.PHN',\n",
       "  'timit/data/TRAIN/DR4/MCSS0/SX210.WRD'),\n",
       " ('timit/data/TRAIN/DR4/MARW0/SX376.WAV',\n",
       "  'timit/data/TRAIN/DR4/MARW0/SX376.PHN',\n",
       "  'timit/data/TRAIN/DR4/MARW0/SX376.WRD'),\n",
       " ('timit/data/TRAIN/DR7/FLEH0/SX61.WAV',\n",
       "  'timit/data/TRAIN/DR7/FLEH0/SX61.PHN',\n",
       "  'timit/data/TRAIN/DR7/FLEH0/SX61.WRD'),\n",
       " ('timit/data/TRAIN/DR5/FBMH0/SX146.WAV',\n",
       "  'timit/data/TRAIN/DR5/FBMH0/SX146.PHN',\n",
       "  'timit/data/TRAIN/DR5/FBMH0/SX146.WRD'),\n",
       " ('timit/data/TRAIN/DR3/MDTB0/SX210.WAV',\n",
       "  'timit/data/TRAIN/DR3/MDTB0/SX210.PHN',\n",
       "  'timit/data/TRAIN/DR3/MDTB0/SX210.WRD'),\n",
       " ('timit/data/TRAIN/DR7/MDPB0/SX146.WAV',\n",
       "  'timit/data/TRAIN/DR7/MDPB0/SX146.PHN',\n",
       "  'timit/data/TRAIN/DR7/MDPB0/SX146.WRD'),\n",
       " ('timit/data/TRAIN/DR5/MPMB0/SX61.WAV',\n",
       "  'timit/data/TRAIN/DR5/MPMB0/SX61.PHN',\n",
       "  'timit/data/TRAIN/DR5/MPMB0/SX61.WRD')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load('processed_data/train_test_dataset_never.joblib')['test'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 11])\n"
     ]
    }
   ],
   "source": [
    "class DNN_FC(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "        # assign layer objects to class attributes\n",
    "        # We may write a loop if we use the same activation function for all layers.\n",
    "        self.fc1 = nn.Linear(input_size, input_size)\n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        self.fc2 = nn.Linear(input_size, input_size)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight) \n",
    "        self.fc3 = nn.Linear(input_size, input_size)\n",
    "        nn.init.kaiming_normal_(self.fc3.weight)\n",
    "        self.fc4 = nn.Linear(input_size, input_size)\n",
    "        nn.init.kaiming_normal_(self.fc4.weight)\n",
    "        self.fc5 = nn.Linear(input_size, num_classes)\n",
    "        nn.init.kaiming_normal_(self.fc5.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_temp = x\n",
    "        x_temp = flatten(x_temp)\n",
    "        x_temp = F.relu(self.fc1(x_temp))\n",
    "        x_temp = F.relu(self.fc2(x_temp))\n",
    "        x_temp = F.relu(self.fc3(x_temp))\n",
    "        x_temp = F.relu(self.fc4(x_temp))\n",
    "        scores = self.fc5(x_temp)\n",
    "        return scores\n",
    "\n",
    "\n",
    "def test_DNN_FC():\n",
    "    input_size = 20  # Feature dimension for mfcc\n",
    "    num_classes = 11 # Number of phoneme classes\n",
    "    dtype = torch.float32\n",
    "    x = torch.zeros((minibatch_size, input_size), dtype=dtype)  # minibatch size 64, feature dimension 20\n",
    "    model = DNN_FC(input_size, num_classes)\n",
    "    scores = model(x)\n",
    "    print(scores.size())  # you should see [minibatch_size, num_classes]\n",
    "test_DNN_FC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#mfcc = df_train.iloc[0:5]['mfcc']\n",
    "#mfcc = np.vstack(mfcc)\n",
    "#mfcc \n",
    "\n",
    "\n",
    "labels = df_train.iloc[0:5]['label']\n",
    "labels = np.vstack(labels)\n",
    "display(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-8.6141e+02,  2.5538e+01,  2.8655e+01,  1.8917e+00,  2.0691e+01,\n",
       "           1.7659e+01,  1.6280e+01,  1.7272e+00,  1.8124e+01,  8.3629e+00,\n",
       "           1.0131e+01,  6.0546e+00, -3.3920e+00,  3.4839e+00,  1.1448e+01,\n",
       "           5.1237e-01,  6.1350e+00,  3.2503e+00,  9.1858e+00,  1.1667e+01],\n",
       "         [-8.2495e+02,  2.0405e+01,  1.5509e+01, -2.5023e+00,  2.0970e+01,\n",
       "           1.8829e+01,  5.9126e+00,  3.8527e+00,  1.6322e+01,  6.7955e+00,\n",
       "           1.4075e+01,  6.0702e+00, -6.9586e+00,  3.7661e+00,  1.0885e+01,\n",
       "           8.5535e+00,  1.2061e+01,  4.0659e+00,  6.1298e+00,  1.0815e+01],\n",
       "         [-8.1813e+02,  1.6537e+01,  1.0081e+01, -9.4607e+00,  1.3550e+01,\n",
       "           1.4193e+01,  1.7715e+00,  9.0556e+00,  2.0455e+01,  9.8571e+00,\n",
       "           1.8602e+01,  9.4670e+00, -6.6779e+00,  2.9148e+00,  5.4056e+00,\n",
       "           1.0185e+01,  8.6857e+00,  2.8653e+00,  4.7726e+00,  7.7956e+00],\n",
       "         [-8.1631e+02,  1.4218e+01,  1.5506e+01, -9.5495e+00,  1.0859e+01,\n",
       "           5.1730e+00, -1.1560e+00,  7.0657e+00,  2.4370e+01,  1.6980e+01,\n",
       "           1.7021e+01,  7.9839e+00, -1.8700e+00,  2.8383e+00,  6.1107e+00,\n",
       "           1.4157e+01,  8.3667e+00,  5.1099e+00,  1.8837e+00,  8.2455e+00],\n",
       "         [-8.0908e+02,  1.7430e+01,  1.4575e+01, -1.0453e+01,  8.5708e+00,\n",
       "           6.6122e+00, -1.2687e+00,  2.8609e+00,  1.5493e+01,  8.4423e+00,\n",
       "           6.9053e+00,  1.1445e+01,  6.7673e+00, -2.2838e+00, -7.8069e-01,\n",
       "           2.3171e+01,  1.4605e+01,  9.5590e+00, -2.0057e+00,  1.6392e+00]]),\n",
       " tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert dataset into a format that torch can read.\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None, train=True):\n",
    "        self.df = dataframe\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the rows that are selected by the idx.\n",
    "        mfcc = self.df.iloc[idx]['mfcc']\n",
    "        label = self.df.iloc[idx]['label']\n",
    "\n",
    "        # Stack the rows for mfcc and label.\n",
    "        # Stack a list of (1,n) dimensional np.ndarrays into (m,n) dimensional np.ndarray. \n",
    "        mfcc = np.vstack(mfcc)\n",
    "        label = np.vstack(label)\n",
    "\n",
    "        # Convert 2 dimensional np.ndarrays into torch tensors.\n",
    "        mfcc = torch.tensor(mfcc, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "        if self.transform:\n",
    "            mfcc = self.transform(mfcc)\n",
    "\n",
    "        return mfcc, label\n",
    "\n",
    "# Create an instance of your dataset with your DataFrame\n",
    "dataset_train = CustomDataset(df_train,train=True)  # Assuming df is your pandas DataFrame\n",
    "dataset_test = CustomDataset(df_test,train=False)\n",
    "\n",
    "# Create the DataLoader to handle batching\n",
    "loader_train = DataLoader(dataset_train, batch_size=minibatch_size,\n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "# Create the DataLoader to handle batching\n",
    "loader_val = DataLoader(dataset_train, batch_size=1,\n",
    "                        sampler=sampler.SequentialSampler(range(NUM_TRAIN, len(df_train))))\n",
    "\n",
    "loader_test = DataLoader(dataset_test, batch_size=1,\n",
    "                        sampler=sampler.SequentialSampler(range(len(df_test))))\n",
    "\n",
    "\n",
    "display(dataset_test.__getitem__(range(5))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Features (MFCCs) size: torch.Size([1, 20, 1])\n",
      "Labels size: torch.Size([1, 14, 1])\n",
      "\n",
      "\n",
      "Batch 2\n",
      "Features (MFCCs) size: torch.Size([1, 20, 1])\n",
      "Labels size: torch.Size([1, 14, 1])\n",
      "\n",
      "\n",
      "Batch 3\n",
      "Features (MFCCs) size: torch.Size([1, 20, 1])\n",
      "Labels size: torch.Size([1, 14, 1])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example code to print the contents of the first few batches in loader_train\n",
    "for i, (inputs, labels) in enumerate(loader_test):\n",
    "    print(f\"Batch {i + 1}\")\n",
    "    print(\"Features (MFCCs) size:\", inputs.size())\n",
    "    print(\"Labels size:\", labels.size())\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Optional: Stop after a few batches to avoid flooding the output\n",
    "    if i == 2:  # Adjust this number based on how many batches you want to see\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            y = flatten(y) # Flatten y to convert dimension from (Nx1) to (N,)\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=dtype) \n",
    "            scores = model(x) \n",
    "            _, preds = scores.max(1) \n",
    "            true_class = y.argmax(dim=1) # True class is the one that has the highest probability in the data.\n",
    "            num_correct += (preds == true_class).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "    return acc \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "dtype = torch.float32\n",
    "\n",
    "def train(model, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    Train the model using the PyTorch Module API.\n",
    "\n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "\n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    accuracy_val_lst = []\n",
    "    accuracy_cal_max = 0\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            y = flatten(y) # Flatten y to convert the dimension from (Nx1) to (N,)\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=dtype)\n",
    "\n",
    "            scores = model(x)\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "            loss = criterion(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "            \n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                print()\n",
    "                accuracy_val = check_accuracy(loader_val, model)\n",
    "                if accuracy_val > accuracy_cal_max:\n",
    "                    accuracy_cal_max = accuracy_val\n",
    "                    model_params = model.state_dict()\n",
    "                accuracy_val_lst.append((t,accuracy_val))\n",
    "        \n",
    "    print('Training is complete. Accuracies on the validation set are:') \n",
    "    print(accuracy_val_lst)\n",
    "    return \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 38.2585\n",
      "\n",
      "Checking accuracy on validation set\n",
      "Got 2 / 3099 correct (0.06)\n",
      "Iteration 0, loss = 0.1570\n",
      "\n",
      "Checking accuracy on validation set\n",
      "Got 1694 / 3099 correct (54.66)\n",
      "Iteration 0, loss = 0.0808\n",
      "\n",
      "Checking accuracy on validation set\n",
      "Got 1980 / 3099 correct (63.89)\n",
      "Iteration 0, loss = 0.0716\n",
      "\n",
      "Checking accuracy on validation set\n",
      "Got 2001 / 3099 correct (64.57)\n",
      "Iteration 0, loss = 0.0757\n",
      "\n",
      "Checking accuracy on validation set\n",
      "Got 1996 / 3099 correct (64.41)\n",
      "Iteration 0, loss = 0.0737\n",
      "\n",
      "Checking accuracy on validation set\n",
      "Got 2022 / 3099 correct (65.25)\n",
      "Iteration 0, loss = 0.0835\n",
      "\n",
      "Checking accuracy on validation set\n",
      "Got 1997 / 3099 correct (64.44)\n",
      "Iteration 0, loss = 0.0751\n",
      "\n",
      "Checking accuracy on validation set\n",
      "Got 2031 / 3099 correct (65.54)\n",
      "Iteration 0, loss = 0.0739\n",
      "\n",
      "Checking accuracy on validation set\n",
      "Got 2023 / 3099 correct (65.28)\n",
      "Iteration 0, loss = 0.0686\n",
      "\n",
      "Checking accuracy on validation set\n",
      "Got 2026 / 3099 correct (65.38)\n",
      "Training is complete. Accuracies on the validation set are:\n",
      "[(0, 0.0006453694740238787), (0, 0.5466279444982253), (0, 0.6389157792836399), (0, 0.6456921587608906), (0, 0.6440787350758309), (0, 0.6524685382381413), (0, 0.6444014198128428), (0, 0.6553727008712488), (0, 0.6527912229751532), (0, 0.6537592771861891)]\n"
     ]
    }
   ],
   "source": [
    "input_size = len(df_train['mfcc'][0])\n",
    "num_classes = len(df_train['label'][0])\n",
    "learning_rate = 1e-2\n",
    "model = DNN_FC(input_size, num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "train(model, optimizer,epochs = 10) \n",
    "\n",
    "# Accuracy on the validation set: 72.57%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_probabilities(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print('Getting estimated probabilities on validation set')\n",
    "    else:\n",
    "        print('Getting estimated probabilities on test set') \n",
    "    model.eval()  # set model to evaluation mode\n",
    "    probabilities_dict = {} \n",
    "    batch_size = loader.batch_size\n",
    "    with torch.no_grad():\n",
    "        for idx, (x, y) in enumerate(loader):\n",
    "            y = flatten(y) # Flatten y to convert dimension from (Nx1) to (N,)\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=dtype) \n",
    "            scores = model(x) \n",
    "            probabilities = torch.softmax(scores, dim=1) \n",
    "            \n",
    "            # Save the probabilities with the corresponding row index\n",
    "            for i in range(len(probabilities)):\n",
    "                probabilities_dict[idx * batch_size + i] = probabilities[i].numpy()\n",
    "    \n",
    "    return probabilities_dict\n",
    "\n",
    "\n",
    "def find_emission(loader, model):\n",
    "    '''\n",
    "    Find emission probabilities for a given data loader and model.\n",
    "    Consider changing this function if it takes too long. Currently: O(n)\n",
    "    '''\n",
    "    # Get the inferred probabilities for each class (12 states, background and silence)\n",
    "    probabilities_dict = infer_probabilities(loader, model) \n",
    "    emission = probabilities_dict\n",
    "    # Get the prior vector and the transition probabilities. We don't need the transition probabilities.\n",
    "    prior_vector, _ = get_prob.main(rerun=False) \n",
    "\n",
    "    # For each key=row_idx and val=prob_array, convert the inferred probabilities into emission.\n",
    "    for key, val in emission.items():\n",
    "        # Slice val to exclude the probabilities for background and silence.\n",
    "        log_prob = np.where(val > 0, np.log(val), -np.inf)   # Get the log probabilities. \n",
    "        log_prob = log_prob[:-2]  # Exclude the background and silence in the emission probability calculation. \n",
    "        emission[key] = [log_prob-prior_vector]  # Divide by prior vector in the log space. \n",
    "    \n",
    "    emission_df = pd.DataFrame.from_dict(emission, orient='index', columns=['Emission']) \n",
    "    return emission_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting estimated probabilities on test set\n",
      "Getting estimated probabilities on test set\n"
     ]
    }
   ],
   "source": [
    "estimate_prob = infer_probabilities(loader_test, model)\n",
    "emission_data = find_emission(loader_test, model)\n",
    "\n",
    "prior_vector, _ = get_prob.main(rerun=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting estimated probabilities on test set\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emission</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-15.582437007565192, -24.97697559242876, -15....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-21.774905650753666, -28.727770956808644, -13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-22.352752177853276, -28.232329520041066, -12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-24.73541208996265, -31.144221457296926, -14....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-23.565106837887456, -30.57955470925005, -14....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>[-14.78414294018238, -23.28407588844927, -15.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>[-11.245132892269782, -19.383465918356496, -12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>[-9.937634914059332, -17.298281821066457, -10....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>[-12.550544231075934, -20.39851108437212, -12....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>[-8.288329570431403, -15.596812399679738, -11....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>407 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Emission\n",
       "0    [-15.582437007565192, -24.97697559242876, -15....\n",
       "1    [-21.774905650753666, -28.727770956808644, -13...\n",
       "2    [-22.352752177853276, -28.232329520041066, -12...\n",
       "3    [-24.73541208996265, -31.144221457296926, -14....\n",
       "4    [-23.565106837887456, -30.57955470925005, -14....\n",
       "..                                                 ...\n",
       "402  [-14.78414294018238, -23.28407588844927, -15.1...\n",
       "403  [-11.245132892269782, -19.383465918356496, -12...\n",
       "404  [-9.937634914059332, -17.298281821066457, -10....\n",
       "405  [-12.550544231075934, -20.39851108437212, -12....\n",
       "406  [-8.288329570431403, -15.596812399679738, -11....\n",
       "\n",
       "[407 rows x 1 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def path_to_emission(file_path_wav: str, file_path_phn: str):\n",
    "    '''\n",
    "    Given the path of a file, get the emission probabilities.\n",
    "    Args:\n",
    "        file_path: Path of the audio file as a string.\n",
    "    Returns:\n",
    "        emit: pd.dataframe\n",
    "            Emission probabilities for each frame in the audio file.\n",
    "    '''\n",
    "    df_test = mfcc_label.prepare_data(file_path_phn,file_path_wav)\n",
    "    column_str_to_numpy(df_test, 'mfcc')\n",
    "    column_str_to_numpy(df_test, 'label')\n",
    "    # Convert dataframe into a loader so that torch can work with.\n",
    "    dataset_test = CustomDataset(df_test,train=False)\n",
    "    loader_test = DataLoader(dataset_test, batch_size=1,\n",
    "                        sampler=sampler.SequentialSampler(range(len(df_test))))\n",
    "\n",
    "    emission_data = find_emission(loader_test, model)\n",
    "    return emission_data\n",
    "    \n",
    "\n",
    "path_to_emission('timit/data/TRAIN/DR4/MDCD0/SX425.WAV','timit/data/TRAIN/DR4/MDCD0/SX425.PHN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting estimated probabilities on test set\n",
      "Getting estimated probabilities on test set\n",
      "Getting estimated probabilities on test set\n",
      "Getting estimated probabilities on test set\n",
      "For start 49040 and end 49440, there is no time-overlapping row.\n",
      "Caution: There is no time-overlapping rows for start 49040 and end 49440\n",
      "Getting estimated probabilities on test set\n",
      "Getting estimated probabilities on test set\n",
      "Getting estimated probabilities on test set\n",
      "For start 62960 and end 63360, there is no time-overlapping row.\n",
      "Caution: There is no time-overlapping rows for start 62960 and end 63360\n",
      "Getting estimated probabilities on test set\n",
      "Getting estimated probabilities on test set\n",
      "For start 43840 and end 44240, there is no time-overlapping row.\n",
      "Caution: There is no time-overlapping rows for start 43840 and end 44240\n",
      "Getting estimated probabilities on test set\n",
      "For start 60640 and end 61040, there is no time-overlapping row.\n",
      "Caution: There is no time-overlapping rows for start 60640 and end 61040\n",
      "Getting estimated probabilities on test set\n",
      "For start 46800 and end 47200, there is no time-overlapping row.\n",
      "Caution: There is no time-overlapping rows for start 46800 and end 47200\n",
      "Getting estimated probabilities on test set\n",
      "Getting estimated probabilities on test set\n",
      "Getting estimated probabilities on test set\n",
      "Getting estimated probabilities on test set\n",
      "Getting estimated probabilities on test set\n",
      "For start 56240 and end 56640, there is no time-overlapping row.\n",
      "Caution: There is no time-overlapping rows for start 56240 and end 56640\n",
      "Getting estimated probabilities on test set\n",
      "For start 56240 and end 56640, there is no time-overlapping row.\n",
      "Caution: There is no time-overlapping rows for start 56240 and end 56640\n",
      "Getting estimated probabilities on test set\n",
      "Getting estimated probabilities on test set\n",
      "For start 31760 and end 32160, there is no time-overlapping row.\n",
      "Caution: There is no time-overlapping rows for start 31760 and end 32160\n",
      "Getting estimated probabilities on test set\n",
      "Getting estimated probabilities on test set\n",
      "Getting estimated probabilities on test set\n",
      "Getting estimated probabilities on test set\n",
      "Getting estimated probabilities on test set\n",
      "For start 52240 and end 52640, there is no time-overlapping row.\n",
      "Caution: There is no time-overlapping rows for start 52240 and end 52640\n",
      "Getting estimated probabilities on test set\n",
      "Getting estimated probabilities on test set\n",
      "For start 0 and end 400, there is no time-overlapping row.\n",
      "For start 80 and end 480, there is no time-overlapping row.\n",
      "For start 160 and end 560, there is no time-overlapping row.\n",
      "For start 240 and end 640, there is no time-overlapping row.\n",
      "For start 320 and end 720, there is no time-overlapping row.\n",
      "For start 400 and end 800, there is no time-overlapping row.\n",
      "For start 480 and end 880, there is no time-overlapping row.\n",
      "For start 560 and end 960, there is no time-overlapping row.\n",
      "For start 640 and end 1040, there is no time-overlapping row.\n",
      "For start 720 and end 1120, there is no time-overlapping row.\n",
      "For start 800 and end 1200, there is no time-overlapping row.\n",
      "For start 880 and end 1280, there is no time-overlapping row.\n",
      "For start 960 and end 1360, there is no time-overlapping row.\n",
      "For start 1040 and end 1440, there is no time-overlapping row.\n",
      "For start 1120 and end 1520, there is no time-overlapping row.\n",
      "For start 1200 and end 1600, there is no time-overlapping row.\n",
      "For start 1280 and end 1680, there is no time-overlapping row.\n",
      "For start 1360 and end 1760, there is no time-overlapping row.\n",
      "For start 1440 and end 1840, there is no time-overlapping row.\n",
      "For start 1520 and end 1920, there is no time-overlapping row.\n",
      "For start 1600 and end 2000, there is no time-overlapping row.\n",
      "For start 1680 and end 2080, there is no time-overlapping row.\n",
      "For start 1760 and end 2160, there is no time-overlapping row.\n",
      "For start 1840 and end 2240, there is no time-overlapping row.\n",
      "For start 1920 and end 2320, there is no time-overlapping row.\n",
      "For start 2000 and end 2400, there is no time-overlapping row.\n",
      "Caution: There is no time-overlapping rows for start 0 and end 400\n",
      "Caution: There is no time-overlapping rows for start 80 and end 480\n",
      "Caution: There is no time-overlapping rows for start 160 and end 560\n",
      "Caution: There is no time-overlapping rows for start 240 and end 640\n",
      "Caution: There is no time-overlapping rows for start 320 and end 720\n",
      "Caution: There is no time-overlapping rows for start 400 and end 800\n",
      "Caution: There is no time-overlapping rows for start 480 and end 880\n",
      "Caution: There is no time-overlapping rows for start 560 and end 960\n",
      "Caution: There is no time-overlapping rows for start 640 and end 1040\n",
      "Caution: There is no time-overlapping rows for start 720 and end 1120\n",
      "Caution: There is no time-overlapping rows for start 800 and end 1200\n",
      "Caution: There is no time-overlapping rows for start 880 and end 1280\n",
      "Caution: There is no time-overlapping rows for start 960 and end 1360\n",
      "Caution: There is no time-overlapping rows for start 1040 and end 1440\n",
      "Caution: There is no time-overlapping rows for start 1120 and end 1520\n",
      "Caution: There is no time-overlapping rows for start 1200 and end 1600\n",
      "Caution: There is no time-overlapping rows for start 1280 and end 1680\n",
      "Caution: There is no time-overlapping rows for start 1360 and end 1760\n",
      "Caution: There is no time-overlapping rows for start 1440 and end 1840\n",
      "Caution: There is no time-overlapping rows for start 1520 and end 1920\n",
      "Caution: There is no time-overlapping rows for start 1600 and end 2000\n",
      "Caution: There is no time-overlapping rows for start 1680 and end 2080\n",
      "Caution: There is no time-overlapping rows for start 1760 and end 2160\n",
      "Caution: There is no time-overlapping rows for start 1840 and end 2240\n",
      "Caution: There is no time-overlapping rows for start 1920 and end 2320\n",
      "Caution: There is no time-overlapping rows for start 2000 and end 2400\n",
      "Getting estimated probabilities on test set\n",
      "Getting estimated probabilities on test set\n",
      "Getting estimated probabilities on test set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['processed_data/test_data_for_hmm.joblib']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_emission_all_paths(path_type: str = 'test'):\n",
    "    paths = load('processed_data/train_test_dataset_never.joblib')[path_type]\n",
    "    data = {}\n",
    "    for i in range(len(paths)):\n",
    "        file_path_wav, file_path_phn, file_path_word = paths[i]\n",
    "        emission_data = path_to_emission(file_path_wav, file_path_phn)\n",
    "        data[(file_path_wav, file_path_phn, file_path_word)] = emission_data\n",
    "\n",
    "    return data\n",
    "data = get_emission_all_paths() \n",
    "\n",
    "from joblib import dump\n",
    "dump(data, \"processed_data/test_data_for_hmm.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
